<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Node Experiment - Exploring Async Basics with Rust</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body class="light">
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            document.body.className = theme;
            document.querySelector('html').className = theme + ' js';
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded affix "><a href="introduction.html">Introduction</a></li><li class="expanded "><a href="1_concurrent_vs_parallel.html"><strong aria-hidden="true">1.</strong> Concurrent vs Parallel</a></li><li class="expanded "><a href="2_async_history.html"><strong aria-hidden="true">2.</strong> Async history</a></li><li class="expanded "><a href="3_0_the_operating_system.html"><strong aria-hidden="true">3.</strong> The Operating System and CPU</a></li><li><ol class="section"><li class="expanded "><a href="3_1_communicating_with_the_os.html"><strong aria-hidden="true">3.1.</strong> Communicating with the OS</a></li><li class="expanded "><a href="3_2_cross_platform_abstractions.html"><strong aria-hidden="true">3.2.</strong> Writing Cross Platform Abstractions</a></li><li class="expanded "><a href="3_3_the_cpu_and_the_os.html"><strong aria-hidden="true">3.3.</strong> The CPU and the OS</a></li></ol></li><li class="expanded "><a href="4_interrupts_firmware_io.html"><strong aria-hidden="true">4.</strong> Interrupts, Firmware and I/O</a></li><li class="expanded "><a href="5_strategies_for_handling_io.html"><strong aria-hidden="true">5.</strong> Strategies for handling I/O</a></li><li class="expanded "><a href="6_epoll_kqueue_iocp.html"><strong aria-hidden="true">6.</strong> Epoll, Kqueue and IOCP</a></li><li class="expanded "><a href="7_0_introducing_our_main_example.html"><strong aria-hidden="true">7.</strong> Introducing our main example</a></li><li><ol class="section"><li class="expanded "><a href="7_1_what_is_node.html"><strong aria-hidden="true">7.1.</strong> What is Node?</a></li><li class="expanded "><a href="7_2_whats_our_plan.html"><strong aria-hidden="true">7.2.</strong> What's our plan</a></li></ol></li><li class="expanded "><a href="8_0_implementing_our_own_runtime.html"><strong aria-hidden="true">8.</strong> Implementing our own Runtime</a></li><li><ol class="section"><li class="expanded "><a href="8_1_the_main_loop.html"><strong aria-hidden="true">8.1.</strong> Running our runtime - the main loop</a></li><li class="expanded "><a href="8_2_setting_up_runtime.html"><strong aria-hidden="true">8.2.</strong> Setting up our runtime</a></li><li class="expanded "><a href="8_3_timers.html"><strong aria-hidden="true">8.3.</strong> Timers</a></li><li class="expanded "><a href="8_4_callbacks.html"><strong aria-hidden="true">8.4.</strong> Callbacks</a></li><li class="expanded "><a href="8_5_threadpool.html"><strong aria-hidden="true">8.5.</strong> Threadpool</a></li><li class="expanded "><a href="8_6_io_eventqueue.html"><strong aria-hidden="true">8.6.</strong> I/O eventqueue</a></li><li class="expanded "><a href="8_8_cleaning_up.html"><strong aria-hidden="true">8.7.</strong> Cleaning up</a></li><li class="expanded "><a href="8_9_infrastructure.html"><strong aria-hidden="true">8.8.</strong> Infrastructure</a></li></ol></li><li class="expanded "><a href="9_0_modules.html"><strong aria-hidden="true">9.</strong> Modules</a></li><li><ol class="section"><li class="expanded "><a href="9_1_file_module.html"><strong aria-hidden="true">9.1.</strong> File module</a></li><li class="expanded "><a href="9_2_crypto_module.html"><strong aria-hidden="true">9.2.</strong> Crypto module</a></li><li class="expanded "><a href="9_3_http_module.html"><strong aria-hidden="true">9.3.</strong> Http module</a></li></ol></li><li class="expanded "><a href="10_putting_pieces_together.html"><strong aria-hidden="true">10.</strong> Putting the pieces together</a></li><li class="expanded "><a href="11_final_code.html"><strong aria-hidden="true">11.</strong> Final code</a></li><li class="expanded "><a href="12_shortcuts_and_improvements.html"><strong aria-hidden="true">12.</strong> Shortcuts and improvements</a></li><li class="expanded affix "><a href="conclusion.html">Conclusion</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">The Node Experiment - Exploring Async Basics with Rust</h1>

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                            <a href="https://github.com/cfsamson/book-exploring-async-basics" title="Git repository" aria-label="Git repository">
                                <i id="git-repository-button" class="fa fa-github"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p><strong>Don't block the event loop! Don't poll in a busy loop! Increase throughput! Use async I/O! Concurrency is not parallelism!</strong></p>
<p>You've most likely heard and read claims like these many times before,
and maybe, at some point you've thought you understood everything
only to find yourself confused a moment later. Especially when you want to
understand how it works on a fundamental level.</p>
<p><strong>Me too.</strong></p>
<p>So I spent a couple of hundred hours trying to fix that for myself. I wrote
this book as a result of that research, and now I invite you to join me as we try
to unveil the secrets of async programming.</p>
<p>This book aims to take a look at the <strong>why</strong> and <strong>how</strong> of concurrent programming. First we build
a good foundation of basic knowledge, before we use that knowledge to investigate how Node.js works
by building a Node-inspired runtime.</p>
<blockquote>
<p>This book is developed in the open and has <a href="https://github.com/cfsamson/book-exploring-async-basics">its repository here</a>.
The book and the <a href="https://github.com/cfsamson/examples-node-eventloop">accompanying code</a> is MIT licensed so feel free to clone away
and play with it.</p>
</blockquote>
<p>I warn you though, we need to venture from philosophical heights where we try to
formally define a &quot;task&quot; all the way down to the deep waters where firmware and
other strange creatures rule (I believe some of the more wicked creatures there
are tasked with naming low level OS syscalls and structures on Windows. However, I
have yet to confirm this).</p>
<blockquote>
<p>Everything in this book will cover the topics for the three major Operating Systems
Linux, macOS and Windows. We'll also only cover the details on how this works
on 64 bit systems.</p>
</blockquote>
<h2><a class="header" href="#who-is-this-book-for" id="who-is-this-book-for">Who is this book for?</a></h2>
<p>I originally started out wanting to explore the fundamentals and inner workings
of Rust's Futures. Reading through RFCs, motivations and discussions I realized
that to really understand the <strong>why</strong> and <strong>how</strong> of Rust's Futures, I needed a very good
understanding of how async code works in general, and the different strategies to handle it.</p>
<p><strong>This book might be interesting if you:</strong></p>
<ul>
<li>
<p>Want to take a deep dive into what concurrency is and strategies on how to deal with it</p>
</li>
<li>
<p>Are curious on how to make syscalls on three different platforms, and do it on three different abstraction levels.</p>
</li>
<li>
<p>Want to know more about how the OS, CPU and hardware handles concurrency.</p>
</li>
<li>
<p>Want to learn the basics of Epoll, Kqueue and IOCP.</p>
</li>
<li>
<p>Think using our research to write a <strong>toy</strong> node.js runtime is pretty cool.</p>
</li>
<li>
<p>Want to know more about what the Node event loop really is, and why most diagrams of it on the web are pretty misleading.</p>
</li>
<li>
<p>Already know some Rust but want to learn more.</p>
</li>
</ul>
<p>So, what do you think? Is the answer yes to some of these questions? Well, then join me on this venture
as we try to get a better understanding of all these subjects.</p>
<blockquote>
<p>We'll only use Rust's standard library. The reason for this is that we really want to know how things
work, and Rust's standard library strikes the perfect balance for this task providing abstractions
but they're thin enough to let us easily peek under the covers and see what really happens.</p>
</blockquote>
<h2><a class="header" href="#following-along" id="following-along">Following along</a></h2>
<p>Even though I use <code>mdbook</code>, which has the nice benefit of being able to run
the code we write directly in the book, we're working with I/O and cross
platform syscalls in this book which is not a good fit for the Rust playground.</p>
<p>My best recommendation is to create a project on your local
computer and follow along by copying the code over and run it locally.</p>
<p><a href="https://github.com/cfsamson/examples-node-eventloop">You can also clone or download the example code from the git repository</a></p>
<h2><a class="header" href="#prerequisites" id="prerequisites">Prerequisites</a></h2>
<p>You don't have to be a Rust programmer to follow along. This book will have numerous chapters where
we explore concepts, and where the code examples are small and easy to understand, but it will
be more code towards the end and you'll get the most out of it by learning the basics first. In this case <a href="https://doc.rust-lang.org/book/title-page.html">The Rust Programming Language</a> is the best place to start.</p>
<p>I do recommend that you read my book preceding this <a href="https://app.gitbook.com/@cfsamson/s/green-threads-explained-in-200-lines-of-rust/">Green threads explained in 200 lines of Rust</a>
since I cover quite a bit about Rust basics, stacks, threads and inline assembly there and
will not repeat everything here. However, it's definitely not a must.</p>
<blockquote>
<p><a href="https://www.rust-lang.org/tools/install">You will find everything you need to set up Rust here</a></p>
</blockquote>
<h2><a class="header" href="#disclaimer" id="disclaimer">Disclaimer</a></h2>
<ol>
<li>
<p>We'll implement a <strong>toy</strong> version of the Node.js event loop (a bad, but working and conceptually similar event loop)</p>
</li>
<li>
<p>We'll <strong>not</strong> primarily focus on code quality and safety.
I will focus on understanding the concepts and ideas behind the code. We will have to make
many shortcuts to keep this concise and short.</p>
</li>
<li>
<p>I will however do my best to point out hazards and the shortcuts we make.
I will try to point out obvious places we could do a better job or take big
shortcuts.</p>
</li>
</ol>
<blockquote>
<p>Even though we
cover some complex topics we'll have to simplify them significantly to be able
to learn anything from them in a small(ish) book. You can probably spend the better
part of a career becoming an expert in several of the fields we cover, so forgive
me already now for not being able to cover all of them with the precision,
thoroughness and respect they deserve.</p>
</blockquote>
<h2><a class="header" href="#credits" id="credits">Credits</a></h2>
<p>Substantial contributions will be credited here.</p>
<h2><a class="header" href="#contributing" id="contributing">Contributing</a></h2>
<p>I have no other interest in this than to share knowledge that can be hard to
come by and make it easier for the next curious person to understand. If you want to
contribute to make this better there are two places to go:</p>
<ol>
<li><a href="https://github.com/cfsamson/book-exploring-async-basics">The base repo for this book</a> for all feedback and content changes</li>
<li><a href="https://github.com/cfsamson/examples-node-eventloop">The base repo for the code example we use</a> for all improvements to the example code</li>
</ol>
<p>Everything from spelling mistakes to correcting errors or inaccuracies are greatly appreciated. It will only make this book better for the next person reading it.</p>
<h2><a class="header" href="#why-i-wrote-this-and-its-companion-books" id="why-i-wrote-this-and-its-companion-books">Why I wrote this and its companion books</a></h2>
<p>This started as a wish to write an article about Rust's Futures 3.0. The result so far is
3 books about concurrency in general and hopefully, at some point a fourth about Rust's Futures exclusively.</p>
<p>This process has also made me realize why I have vague memories from my childhood
of threats being made about stopping the car and letting me off if I didn't stop
asking &quot;why?&quot; to everything.</p>
<p>Basically, the list below is a result of this urge to understand <em>why</em> while
reading the RFCs and discussions about Rust's async story:</p>
<ul>
<li>
<p><a href="https://app.gitbook.com/@cfsamson/s/green-threads-explained-in-200-lines-of-rust/">Green threads explained in 200 lines of Rust</a></p>
</li>
<li>
<p><a href="https://cfsamson.github.io/book-exploring-async-basics/">The Node Experiment - Exploring Async Basics with Rust</a> (this book)</p>
</li>
<li>
<p><a href="https://github.com/cfsamson/book-exploring-epoll-kqueue-iocp">Exploring Epoll, Kqueue and IOCP with Rust</a> a companion book to the &quot;Async Basics&quot; book</p>
</li>
<li>
<p>Exploring Rust's Futures (TBD) - a different look on the <strong>why</strong> and <strong>how</strong> of Rust's futures</p>
</li>
</ul>
<h1><a class="header" href="#whats-the-difference-between-concurrency-and-parallelism" id="whats-the-difference-between-concurrency-and-parallelism">What's the difference between concurrency and parallelism?</a></h1>
<p>Right off the bat, we'll dive into this subject by defining what concurrency is.
Since it is quite easy to confuse &quot;concurrent&quot; with &quot;parallel&quot;, we will try to make
a clear distinction between the two from the get-go.</p>
<blockquote>
<p>Concurrency is about <strong>dealing</strong> with a lot of things at the same time.</p>
</blockquote>
<blockquote>
<p>Parallelism is about <strong>doing</strong> a lot of things at the same time.</p>
</blockquote>
<p>We call the concept of progressing multiple tasks at the same time <code>Multitasking</code>.
There are two ways to multitask. One is by <strong>progressing</strong> tasks concurrently,
but not at the same time. Another is to progress tasks at the exact same time in parallel.</p>
<p><img src="./images/definitions.jpg" alt="parallel vs concurrency" /></p>
<h2><a class="header" href="#lets-start-off-with-some-definitions" id="lets-start-off-with-some-definitions">Lets start off with some definitions</a></h2>
<h3><a class="header" href="#resource" id="resource">Resource</a></h3>
<p>Something we need to be able to progress a task. Our resources are limited. This
could be CPU time or memory.</p>
<h3><a class="header" href="#task" id="task">Task</a></h3>
<p>A set of operations that requires some kind of resource to progress. A task must
consist of several sub-operations.</p>
<h3><a class="header" href="#parallel" id="parallel">Parallel</a></h3>
<p>Something happening independently at the <strong>exact</strong> same time.</p>
<h3><a class="header" href="#concurrent" id="concurrent">Concurrent</a></h3>
<p>Tasks that are <strong><code>in progress</code></strong> at the same time, but not <em>necessarily</em> progressing
simultaneously.</p>
<p>This is an important distinction. If two tasks are running concurrently,
but are not running in parallel, they must be able to stop and resume their progress.
We say that a task is <code>interruptable</code> if it allows for this kind of concurrency.</p>
<h2><a class="header" href="#the-mental-model-i-use" id="the-mental-model-i-use">The mental model I use.</a></h2>
<p>I firmly believe the main reason we find parallel and concurrent programming hard to reason about stems from how we model events in our everyday life. We tend to define these terms loosely so our intuition is often wrong.</p>
<blockquote>
<p>It doesn't help that <strong>concurrent</strong> is defined in the dictionary as: <em>operating or occurring at the same time</em> which
doesn't really help us much when trying to describe how it differs from <strong>parallel</strong></p>
</blockquote>
<p>For me, this first clicked when I started to understand why we want to make a distinction between parallel and concurrent in the first place!</p>
<p>The <strong>why</strong> has everything to do with resource utilization and <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a>.</p>
<blockquote>
<p>Efficiency is the (often measurable) ability to avoid wasting materials, energy, efforts, money, and time in doing something or in producing a desired result.</p>
</blockquote>
<h3><a class="header" href="#parallelism" id="parallelism">Parallelism</a></h3>
<p>Is increasing the resources we use to solve a task. It has nothing to do with <em>efficiency</em>.</p>
<h3><a class="header" href="#concurrency" id="concurrency">Concurrency</a></h3>
<p>Has everything to do with efficiency and resource utilization. Concurrency can never make <em>one single task go faster</em>.
It can only help us utilize our resources better and thereby <em>finish a set of tasks faster</em>.</p>
<h3><a class="header" href="#lets-draw-some-parallels-to-process-economics" id="lets-draw-some-parallels-to-process-economics">Let's draw some parallels to process economics</a></h3>
<p>In businesses that manufacture goods, we often talk about LEAN processes. And this is pretty easy to compare with why programmers care so much about what we can achieve if we handle tasks concurrently.</p>
<p>I'll let let this 3 minute video explain it for me:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Oz8BR5Lflzg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>OK, so it's not the newest video on the subject, but it explains a lot in 3 minutes. Most importantly the gains we try to achieve when applying LEAN techniques, and most importantly: <strong>eliminate waiting and non-value-adding tasks.</strong></p>
<blockquote>
<p>In programming we could say that we want to avoid <code>blocking</code> and <code>polling</code> (in a busy loop).</p>
</blockquote>
<p>Now would adding more resources (more workers) help in the video above? Yes, but we use double the resources to produce the same output as one person with an optimal process could do. That's not the best utilization of our resources.</p>
<blockquote>
<p>To continue the parallel we started, we could say that we could solve the problem of a freezing UI while waiting for an I/O event to occur
by spawning a new thread and <code>poll</code> in a loop or <code>block</code> there instead of our main thread. However, that new
thread is either consuming resources doing nothing, or worse, using one core to busy loop while checking if
an event is ready. Either way, it's not optimal, especially if you run a server you want to utilize fully.</p>
</blockquote>
<p>If you consider the coffee machine as some I/O resource, we would like to start that process, then move on to preparing the
next job, or do other work that needs to be done instead of waiting.</p>
<p><em>But that means there are things happening in parallel here?</em></p>
<p>Yes, the coffee machine is doing work while the &quot;worker&quot; is doing
maintenance and filling water. But this is the crux: <em>Our reference frame is the worker, not the whole system. The guy making coffee is your code.</em></p>
<blockquote>
<p>It's the same when you make a database query. After you've sent the query to the database server,
the CPU on the database server will be working on your request while you wait for a response. In practice, it's a way
of parallelizing your work.</p>
</blockquote>
<p><strong>Concurrency is about working smarter. Parallelism is a way of throwing more resources at the problem.</strong></p>
<h2><a class="header" href="#concurrency-and-its-relation-to-io" id="concurrency-and-its-relation-to-io">Concurrency and its relation to I/O</a></h2>
<p>As you might understand from what I've written so far, writing async code mostly
makes sense when you need to be smart to make optimal use of your resources.</p>
<p>Now, if you write a program that is working hard to solve a problem, there often is no help
in concurrency, this is where parallelism comes into play since it gives you
a way to throw more resources at the problem if you can split it into parts that
you can work on in parallel.</p>
<p><strong>I can see two major use cases for concurrency:</strong></p>
<ol>
<li>When performing I/O and you need to wait for some external event to occur</li>
<li>When you need to divide your attention and prevent one task from waiting too long</li>
</ol>
<p>The first is the classic I/O example: you have to wait for a network
call, a database query or something else to happen before you can progress a
task. However, you have many tasks to do so instead of waiting you continue work
elsewhere and either check in regularly to see if the task is ready to progress
or make sure you are notified when that task is ready to progress.</p>
<p>The second is an example that is often the case when having a UI. Let's pretend
you only have one core. How do you prevent the whole UI from becoming unresponsive
while performing other CPU intensive tasks?</p>
<p>Well, you can stop whatever task you're doing every 16ms, and run the &quot;update UI&quot;
task, and then resume whatever you were doing afterwards. This way, you will have
to stop/resume your task 60 times a second, but you will also have a fully responsive UI which has roughly a 60 Hz refresh rate.</p>
<h2><a class="header" href="#what-about-threads-provided-by-the-os" id="what-about-threads-provided-by-the-os">What about threads provided by the OS?</a></h2>
<p>We'll cover threads a bit more when we talk about strategies for handling I/O, but I'll mention them here as well. One challenge when using OS threads to understand concurrency
is that they appear to be mapped to cores. That's not necessarily a correct mental model
to use even though most operating systems will try to map one thread to one
core up to the number of threads is equal to the number of cores.</p>
<p>Once we create more threads than there are cores, the OS will switch between our
threads and progress each of them <code>concurrently</code> using the scheduler to give each
thread some time to run. And you also have to consider the fact that your program
is not the only one running on the system. Other programs might spawn several threads
as well which means there will be many more threads than there are cores on the CPU.</p>
<p>Therefore, threads can be a means to perform tasks in parallel, but they can also
be a means to achieve concurrency.</p>
<p>This brings me over to the last part about concurrency. It needs to be defined
in some sort of reference frame.</p>
<h2><a class="header" href="#changing-the-reference-frame" id="changing-the-reference-frame">Changing the reference frame</a></h2>
<p>When you write code that is perfectly synchronous from your perspective, stop for a second and consider how that looks from the operating system perspective.</p>
<p>The Operating System might not run your code from start to end at all. It might stop and resume your process many times. The CPU might get interrupted and handle some inputs while you think it's only focused on your task.</p>
<p>So synchronous execution is only an illusion. But from the perspective of you as a programmer, it's not, and that is the important takeaway:</p>
<p><strong>When we talk about concurrency without providing any other context we are using you as a programmer and your code (your process) as the reference frame. If you start pondering about concurrency
without keeping this in the back of your head it will get confusing very fast.</strong></p>
<p>The reason I spend so much time on this is that once you realize that, you'll start to see that some of the things you hear and learn that might seem contradicting really is not. You'll just have to consider the reference frame first.</p>
<p>If this still sounds complicated, I understand. Just sitting and reflecting about concurrency is difficult, but if we try to keep these thoughts in the back of our head when we work with async code I promise it will get less and less confusing.</p>
<h1><a class="header" href="#async-history" id="async-history">Async history</a></h1>
<p>In the beginning, computers had one CPU and it executed a set of instructions written
by a programmer one by one. No scheduling, no threads, no multitasking. This was
how computers worked for a long time. We're talking back when a program
looked like a deck of these:</p>
<p><img src="./images/punched_card_deck.jpg" alt="Image" /></p>
<p>There were operating systems being researched even very early and when personal computing
started to grow in the 80s, operating systems like DOS were the standard on most consumer
PCs.</p>
<p>These operating systems usually yielded control of the entire CPU to the program currently executing and it was
up to the programmer to make things work and implement any kind of multitasking
for their program. This worked fine, but as interactive UIs using a mouse and
windowed operating systems became the norm, this model simply couldn't work anymore.</p>
<h2><a class="header" href="#non-preemptive-multitasking" id="non-preemptive-multitasking">Non-Preemptive multitasking</a></h2>
<p>The first method used to be able to keep a UI interactive (and running background
processes), was accomplished by what we call <code>non-preemptive multitasking</code>.</p>
<p>This kind of multitasking put the responsibility of letting the OS run other tasks like responding to input from the mouse, or running a background task in the hands of the programmer.</p>
<p>Typically the programmer <code>yielded</code> control to the OS.</p>
<p>Besides off-loading a huge responsibility to every programmer writing a program
for your platform, this was naturally error-prone. A small mistake in a programs code
could halt or crash the entire system.</p>
<blockquote>
<p>If you remember Windows 95, you also remember the times when a window hung and you could paint the entire screen with it (almost the same way as the end in Solitaire, the card game that came with Windows).</p>
<p>This was reportedly a typical error in the code that was supposed to yield control to the operating system.</p>
</blockquote>
<h2><a class="header" href="#preemptive-multitasking" id="preemptive-multitasking">Preemptive multitasking</a></h2>
<p>While non-preemptive multitasking sounded like a good idea, it turned out to
create serious problems as well. Letting every program and programmer out there be responsible for having a responsive UI in an operating system can ultimately lead to a bad user experience since every bug out there could halt the entire system.</p>
<p>The solution was to place the responsibility of scheduling the CPU resources
between the programs that requested it (including to OS itself) in the hands of
the OS. The OS can stop the execution of a process, do something else, and switch back.</p>
<p>On a single-core machine, you can visualize this as running a program you wrote,
the OS has to stop your program to update the mouse position before it switches back to your
program to continue. This happens so frequently that we don't observe any difference whether the CPU
has a lot of work or is idle.</p>
<p>The OS is then responsible for scheduling tasks and does this by switching contexts on the CPU. This process can happen many times each second, not only to keep the UI responsive but it can also give some time to other background tasks and IO events.</p>
<p>This is now the prevailing way to design an operating system.</p>
<blockquote>
<p>If you want to learn more about this kind of threaded multitasking I recommend reading my previous book about <a href="https://cfsamson.gitbook.io/green-threads-explained-in-200-lines-of-rust/">green threads</a>. This is a nice introduction where you'll be able to get the basic knowledge you need about threads, contexts, stacks and scheduling.</p>
</blockquote>
<h2><a class="header" href="#hyperthreading" id="hyperthreading">Hyperthreading</a></h2>
<p>As CPUs evolved and added more functionality, such as several ALUs (Arithmetic Logic Units), and additional logic units, the CPU manufacturers realized that the entire CPU was never utilized fully. For example, when an operation only required some parts of the CPU, an instruction could be run on the ALU simultaneously. This became the start of Hyperthreading.</p>
<p>You see, on your computer today that it has i.e. 6 cores, and 12 logical cores.
This is exactly where Hyperthreading comes in. It &quot;simulates&quot; two cores on the
same core by using unused parts of the CPU to drive progress on thread &quot;2&quot;
simultaneously as it's running the code on thread &quot;1&quot;. It does this by using a
number of smart tricks (like the one with the ALU).</p>
<p>Now, having hyperthreading, we could actually offload some work on one thread while keeping the UI
interactive by responding to events in the second thread even though we only
had one CPU core thereby utilizing our hardware better.</p>
<blockquote>
<p>You might wonder about the performance of Hyper Threading?</p>
<p>It turns out that Hyperthreading has been continuously improved since the '90s.
Since you're not actually running two CPU's there will be some operations that
need to wait for each other to finish. The performance gain of hyperthreading
compared to multithreading in a single core seems to be <a href="https://en.wikipedia.org/wiki/Hyper-threading#Performance_claims">somewhere close
to 30 %</a> but
it largely depends on the workload.</p>
</blockquote>
<h2><a class="header" href="#multicore-processors" id="multicore-processors">Multicore processors</a></h2>
<p>As most know, the clock frequency of processors has been flat for a long time.
Processors get faster by improving caches, branch prediction, speculative execution
and working on the processing pipelines of the processors, but the gains seems to
be diminishing.</p>
<p>On the other hand, new processors are so small they allow us to have many on the
same chip instead. Now, most CPUs have many cores, and most often each core will also have the ability to perform hyperthreading.</p>
<h2><a class="header" href="#so-how-synchronous-is-the-code-you-write-really" id="so-how-synchronous-is-the-code-you-write-really">So how synchronous is the code you write, really?</a></h2>
<p>Like many things, this depends on your perspective. From the perspective of your process and the code you write for it, everything will normally happen in the order you write it.</p>
<p>From the OS perspective, it might, or might not, interrupt your code, pause it
and run some other code in the meantime before resuming your process.</p>
<p>From the perspective of the CPU, it will mostly execute instructions one at a time *.
They don't care who wrote the code though so when a hardware interrupt happens,
they will immediately stop and give control to an interrupt handler. This is how
the CPU handles concurrency.</p>
<blockquote>
<p>* However, modern CPUs can also do a lot of things in parallel. Most CPUs are
pipelined, meaning that the next instruction is loaded while the current is
executing. It might have a branch predictor that tries to figure out what
instructions to load next.</p>
<p>The processor can also reorder instructions by using
&quot;out of order execution&quot; if it believes it makes things faster this way without
&quot;asking&quot; or &quot;telling&quot; the programmer or the OS so you might not have any guarantee that A happens before B.</p>
<p>The CPU offloads some work to separate &quot;coprocessors&quot; like the FPU for floating-point calculations leaving the main CPU ready to do other tasks et cetera.</p>
<p>As a high-level overview, it's OK to model the CPU as operating in a synchronous
manner, but lets for now just make a mental note that this is a model with some
caveats that become especially important when talking about parallelism,
synchronization primitives (like mutexes and atomics) and the security of computers
and operating systems.</p>
</blockquote>
<h1><a class="header" href="#the-operating-system" id="the-operating-system">The Operating System</a></h1>
<p>The operating system stands in the centre of everything we do as programmers (well, unless you're <a href="https://os.phil-opp.com/">writing an Operating System</a> or are in the <a href="https://rust-embedded.github.io/book/">Embedded realm</a>),
so there is no way for us to discuss any kind of fundamentals in programming
without talking about operating systems in a bit of detail.</p>
<h2><a class="header" href="#concurrency-from-the-operating-systems-perspective" id="concurrency-from-the-operating-systems-perspective">Concurrency from the operating systems' perspective</a></h2>
<div style="color: back;  font-style: italic; font-size: 1.2em">"Operating systems has been "faking" synchronous execution since the 90s."</div>
<p>This ties into what I talked about in the first chapter when I said that <code>concurrent</code>
needs to be talked about within a reference frame and I explained that the OS
might stop and start your process at any time.</p>
<p>What we call synchronous code is in most cases code that appears as synchronous to us as programmers. Neither the OS or the CPU live in a fully synchronous world.</p>
<p>Operating systems use <code>preemptive multitasking</code> and as long as the operating system you're running is preemptively scheduling processes, you won't have a
guarantee that your code runs instruction by instruction without interruption.</p>
<p>The operating system will make sure that all important processes get some time from the CPU to make progress.</p>
<blockquote>
<p>This is not as simple when we're talking about modern machines with 4-6-8-12
physical cores since you might actually execute code on one of the CPU's
uninterrupted if the system is under very little load. The important part here
is that you can't know for sure and there is no guarantee that your code will be
left to run uninterrupted.</p>
</blockquote>
<h2><a class="header" href="#teaming-up-with-the-os" id="teaming-up-with-the-os">Teaming up with the OS.</a></h2>
<p>When programming it's often easy to forget how many moving pieces that need to
cooperate when we care about efficiency. When you make a web request, you're not
asking the CPU or the network card to do something for you, you're asking the
operating system to talk to the network card for you.</p>
<p>There is no way for you as a programmer to make your system optimally efficient
without playing to the strengths of the operating system. You basically don't have
access to the hardware directly.</p>
<p>However, this also means that to understand everything from the ground up, you'll also need to know how your operating system handles these tasks.</p>
<p>To be able to work with the operating system, we'll need to know how we can communicate with it and that's exactly what we're going to go through next.</p>
<h1><a class="header" href="#communicating-with-the-operating-system" id="communicating-with-the-operating-system">Communicating with the operating system</a></h1>
<p><strong>In this chapter I want to dive into:</strong></p>
<ul>
<li>What a System Call is</li>
<li>Abstraction levels</li>
<li>Challenges of writing low-level cross-platform code</li>
</ul>
<h2><a class="header" href="#syscall-primer" id="syscall-primer">Syscall primer</a></h2>
<p>Communication with the operating system is done through <code>System Calls</code> or
&quot;syscalls&quot; as we'll call them from now on. This is a public API that the operating system provides and that programs we write in &quot;userland&quot; can use to communicate with the OS.</p>
<p>Most of the time these calls are abstracted away for us as programmers by the language or the runtime we use. A language like Rust makes it
trivial to make a <code>syscall</code> though which we'll see below.</p>
<p>Now, <code>syscalls</code> is an example of something that is unique to the kernel you're communicating with, but the UNIX family of kernels has many similarities. UNIX systems expose this through <strong><code>libc</code></strong>.</p>
<p>Windows, on the other hand, uses its own API, often referred to as WinAPI, and that can be radically different from how the UNIX based systems operate.</p>
<p>Most often though there is a way to achieve the same things. In terms of functionality, you might not notice a big difference but as we'll see below and especially when we dig into how <code>epoll</code>, <code>kqueue</code> and <code>IOCP</code> work, they can differ a lot in how this functionality is implemented.</p>
<h2><a class="header" href="#syscall-example" id="syscall-example">Syscall example</a></h2>
<p>To get a bit more familiar with <code>syscalls</code> we'll implement a very basic one for the three architectures: <code>BSD(macos)</code>, <code>Linux</code> and <code>Windows</code>. We'll also see how this is implemented in three levels of abstractions.</p>
<p>The <code>syscall</code> we'll implement is the one used when we write something to <code>stdout</code> since that is such a common operation and it's interesting to see how it really works.</p>
<h3><a class="header" href="#the-lowest-level-of-abstraction" id="the-lowest-level-of-abstraction">The lowest level of abstraction</a></h3>
<p>For this to work we need to write some <a href="https://doc.rust-lang.org/1.0.0/book/inline-assembly.html">inline assembly</a>. We'll start by focusing on the instructions we write to the CPU.</p>
<blockquote>
<p>If you want a more thorough introduction to inline assembly I can refer you to the <a href="https://cfsamson.gitbook.io/green-threads-explained-in-200-lines-of-rust/an-example-we-can-build-upon">relevant chapter in my previous book</a> if you haven't read it already.</p>
</blockquote>
<p>Now at this level of abstraction, we'll write different code for all three platforms.</p>
<p>On Linux and macOS the <code>syscall</code> we want to invoke is called <code>write</code>. Both systems operate based on the concept of <code>file descriptors</code> and <code>stdout</code> is one of these already present when you start a process.</p>
<p><strong>On Linux a <code>write</code> syscall can look like this</strong> <br />
(You can run the example by clicking &quot;play&quot; in the right corner)</p>
<pre><pre class="playpen"><code class="language-rust">#![feature(llvm_asm)]
fn main() {
    let message = String::from(&quot;Hello world from interrupt!\n&quot;);
    syscall(message);
}

#[cfg(target_os = &quot;linux&quot;)]
#[inline(never)]
fn syscall(message: String) {
    let msg_ptr = message.as_ptr();
    let len = message.len();

    unsafe {
        llvm_asm!(&quot;
        mov     $$1, %rax   # system call 1 is write on Linux
        mov     $$1, %rdi   # file handle 1 is stdout
        mov     $0, %rsi    # address of string to output
        mov     $1, %rdx    # number of bytes
        syscall             # call kernel, syscall interrupt
    &quot;
        :
        : &quot;r&quot;(msg_ptr), &quot;r&quot;(len)
        : &quot;rax&quot;, &quot;rdi&quot;, &quot;rsi&quot;, &quot;rdx&quot;
        )
    }
}
</code></pre></pre>
<p>The code to initiate the <code>write</code> syscall on Linux is <code>1</code> so when we write <code>$$1</code> we're writing the literal value 1 to the <code>rax</code> register.</p>
<blockquote>
<p><code>$$</code> in inline assembly using the AT&amp;T syntax is how you write a literal value. A single <code>$</code> means you're referring to a parameter so when we write <code>$0</code> we're referring to the first parameter called <code>msg_ptr</code>. We also need to clobber the registers we write to so that we let the compiler know that we're modifying them and it can't rely on storing any values in these.</p>
</blockquote>
<p>Coincidentally, placing the value <code>1</code> into the <code>rdi</code> register means that we're referring to <code>stdout</code> which is the file descriptor we want to write to. This has nothing to do with the fact that the <code>write</code> syscall also has the code <code>1</code>.</p>
<p>Secondly, we pass in the address of our string buffer and the length of the buffer in the registers <code>rsi</code> and <code>rdx</code> respectively, and call the <code>syscall</code> instruction.</p>
<blockquote>
<p>The <code>syscall</code> instruction is a rather new one. On the earlier 32-bit systems in the <code>x86</code> architecture, you invoked a syscall by issuing a software interrupt <code>int 0x80</code>. A software interrupt is considered slow at the level we're working at here so later a separate instruction for it called <code>syscall</code> was added. The <code>syscall</code> instruction uses <a href="http://articles.manugarg.com/systemcallinlinux2_6.html">VDSO</a>, which is a memory page attached to each process' memory, so no context switch is necessary to execute the system call.</p>
</blockquote>
<p><strong>On macOS, the syscall will look something like this:</strong> <br />
(since the Rust playground is running Linux, we can't run this example here)</p>
<pre><pre class="playpen"><code class="language-rust no_run">#![feature(llvm_asm)]
fn main() {
    let message = String::from(&quot;Hello world from interrupt!\n&quot;);
    syscall(message);
}

#[cfg(target_os = &quot;macos&quot;)]
fn syscall(message: String) {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    unsafe {
        llvm_asm!(
            &quot;
        mov     $$0x2000004, %rax   # system call 0x2000004 is write on macos
        mov     $$1, %rdi           # file handle 1 is stdout
        mov     $0, %rsi            # address of string to output
        mov     $1, %rdx            # number of bytes
        syscall                     # call kernel, syscall interrupt
    &quot;
        :
        : &quot;r&quot;(msg_ptr), &quot;r&quot;(len)
        : &quot;rax&quot;, &quot;rdi&quot;, &quot;rsi&quot;, &quot;rdx&quot;
        )
    };
}
</code></pre></pre>
<p>As you see this is not that different from the one we wrote for Linux, with the exception of the fact that syscall <code>write</code> has the code <code>0x2000004</code> instead of <code>1</code>.</p>
<p><strong>What about Windows?</strong></p>
<p>This is a good opportunity to explain why writing code like we do above is a bad idea.</p>
<p>You see, if you want your code to work for a long time you have to worry about what <code>guarantees</code> the OS gives you. As far as I know, both Linux and macOS give some guarantees that for example <code>$$0x2000004</code> on macOS will always refer to <code>write</code> (I'm not sure how strong these guarantees are though). Windows gives absolutely zero guarantees when it comes to low-level internals like this.</p>
<p>Windows has changed it's internals numerous times and provides no official documentation. The only thing we got is reverse engineered tables like <a href="https://j00ru.vexillium.org/syscalls/nt/64/">this</a>. That means that what was <code>write</code> can be changed to <code>delete</code> the next time you run Windows update.</p>
<h2><a class="header" href="#the-next-level-of-abstraction" id="the-next-level-of-abstraction">The next level of abstraction</a></h2>
<p>The next level of abstraction is to use the API which all three operating systems provide for us.</p>
<p>Already we can see that this abstraction helps us remove some code since fortunately for us, in this specific example, the syscall is the same on Linux and on macOS so we only need to worry if we're on Windows and therefore use the <code>#[cfg(not(target_os = &quot;windows&quot;))]</code> conditional compilation flag. For the Windows syscall, we do the opposite.</p>
<h3><a class="header" href="#using-the-os-provided-api-in-linux-and-macos" id="using-the-os-provided-api-in-linux-and-macos">Using the OS provided API in Linux and macOS</a></h3>
<p>You can run this code directly here in the window. However, the Rust playground
runs on Linux, you'll need to copy the code over to a Windows machine if you
want to try it out the code for Windows further down.</p>
<p><strong>Our syscall will now look like this</strong> <br />
(You can run this code here. It will work for both Linux and macOS)</p>
<pre><pre class="playpen"><code class="language-rust">use std::io;

fn main() {
    let sys_message = String::from(&quot;Hello world from syscall!\n&quot;);
    syscall(sys_message).unwrap();
}

// and: http://man7.org/linux/man-pages/man2/write.2.html
#[cfg(not(target_os = &quot;windows&quot;))]
#[link(name = &quot;c&quot;)]
extern &quot;C&quot; {
    fn write(fd: u32, buf: *const u8, count: usize) -&gt; i32;
}

#[cfg(not(target_os = &quot;windows&quot;))]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    let res = unsafe { write(1, msg_ptr, len) };

    if res == -1 {
        return Err(io::Error::last_os_error());
    }
    Ok(())
}

</code></pre></pre>
<p>I'll explain what we just did here. I assume that the <code>main</code> method needs no comment.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>#[link(name = &quot;c&quot;)]
<span class="boring">}
</span></code></pre></pre>
<p>Every Linux installation comes with a version of <code>libc</code> which is a C-library for communicating with the operating system. Having a <code>libc</code> with a consistent API means they can change the underlying implementation without breaking everyone's code. This flag tells the compiler to link to the &quot;c&quot; library on the system we're compiling for.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>extern &quot;C&quot; {
    fn write(fd: u32, buf: *const u8, count: usize);
}
<span class="boring">}
</span></code></pre></pre>
<p><code>extern &quot;C&quot;</code> or only <code>extern</code> (C is assumed if nothing is specified) means we're linking to specific functions in the &quot;c&quot; library using the &quot;C&quot; calling convention. As you'll see on Windows we'll need to change this since it uses a different calling convention than the UNIX family.</p>
<p>The function we're linking to needs to have the exact same name, in this case, <code>write</code>. The parameters don't need to have the same name but they must be in
the right order and it's good practice to name them the same as in the library
you're linking to.</p>
<p>The write function takes a <code>file descriptor</code> which in this case is a handle to
<code>stdout</code>. In addition, it expects us to provide a pointer to an array of <code>u8</code> values and the length of the buffer.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>#[cfg(not(target_os = &quot;windows&quot;))]
fn syscall_libc(message: String) {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    unsafe { write(1, msg_ptr, len) };
}
<span class="boring">}
</span></code></pre></pre>
<p>First, we get a pointer to the underlying buffer of our
string. This will be a pointer of type <code>*const u8</code> which matches our <code>buf</code>
argument. The <code>len</code> of the buffer corresponds to the <code>count</code> argument.</p>
<p>You might ask how we know that <code>1</code> is the file-handle to <code>stdout</code> and where we found that value.</p>
<p>You'll notice this a lot when writing syscalls from Rust. Usually, constants are defined in the C header files which we can't link to, so we need to search them up. 1 is always the file descriptor for <code>stdout</code> on UNIX systems.</p>
<blockquote>
<p>Wrapping the <code>libc</code> functions and providing these constants is exactly what the
crate <a href="https://github.com/rust-lang/libc">libc</a> provides for us and why you'll see that used instead of writing
the type of code we do here.</p>
</blockquote>
<p>A call to an FFI function is always unsafe so we need to use the <code>unsafe</code> keyword
here.</p>
<h3><a class="header" href="#using-the-api-on-windows" id="using-the-api-on-windows">Using the API on Windows</a></h3>
<p><strong>This syscall will look like this on Windows:</strong> <br />
(You'll need to copy this code over to a Windows machine to try this out)</p>
<pre><pre class="playpen"><code class="language-rust no_run">use std::io;

fn main() {
    let sys_message = String::from(&quot;Hello world from syscall!\n&quot;);
    syscall(sys_message).unwrap();
}

#[cfg(target_os = &quot;windows&quot;)]
#[link(name = &quot;kernel32&quot;)]
extern &quot;stdcall&quot; {
    /// https://docs.microsoft.com/en-us/windows/console/getstdhandle
    fn GetStdHandle(nStdHandle: i32) -&gt; i32;
    /// https://docs.microsoft.com/en-us/windows/console/writeconsole
    fn WriteConsoleW(
        hConsoleOutput: i32,
        lpBuffer: *const u16,
        numberOfCharsToWrite: u32,
        lpNumberOfCharsWritten: *mut u32,
        lpReserved: *const std::ffi::c_void,
    ) -&gt; i32;
}

#[cfg(target_os = &quot;windows&quot;)]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {

    // let's convert our utf-8 to a format windows understands
    let msg: Vec&lt;u16&gt; = message.encode_utf16().collect();
    let msg_ptr = msg.as_ptr();
    let len = msg.len() as u32;

    let mut output: u32 = 0;
        let handle = unsafe { GetStdHandle(-11) };
        if handle  == -1 {
            return Err(io::Error::last_os_error())
        }

        let res = unsafe {
            WriteConsoleW(handle, msg_ptr, len, &amp;mut output, std::ptr::null())
            };
        if res  == 0 {
            return Err(io::Error::last_os_error());
        }

    assert_eq!(output as usize, len);
    Ok(())
}
</code></pre></pre>
<p>Now, just by looking at the code above you see it starts to get a bit more
complex, but let's spend some time to go through line by line what we do here as
well.</p>
<pre><code class="language-text">#[cfg(target_os = &quot;windows&quot;)]
#[link(name = &quot;kernel32&quot;)]
</code></pre>
<p>The first line is just telling the compiler to only compile this if the <code>target_os</code> is Windows.</p>
<p>The second line is a linker directive, telling the linker we want to link to the library <code>kernel32</code> (if you ever see an example that links to <code>user32</code> that will also work).</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>extern &quot;stdcall&quot; {
    /// https://docs.microsoft.com/en-us/windows/console/getstdhandle
    fn GetStdHandle(nStdHandle: i32) -&gt; i32;
    /// https://docs.microsoft.com/en-us/windows/console/writeconsole
    fn WriteConsoleW(
        hConsoleOutput: i32,
        lpBuffer: *const u16,
        numberOfCharsToWrite: u32,
        lpNumberOfCharsWritten: *mut u32,
        lpReserved: *const std::ffi::c_void,
    ) -&gt; i32;
}
<span class="boring">}
</span></code></pre></pre>
<p>First of all, <code>extern &quot;stdcall&quot;</code>, tells the compiler that we won't use the <code>C</code>
calling convention but use Windows calling convention called <code>stdcall</code>.</p>
<p>The next part is the functions we want to link to. On Windows, we need to link to two functions to get this to work: <code>GetStdHandle</code> and <code>WriteConsoleW</code>.
<code>GetStdHandle</code> retrieves a reference to a standard device like <code>stdout</code>.</p>
<p><code>WriteConsole</code> comes in two flavours, <code>WriteConsoleW</code> that takes in Unicode text and <code>WriteConsoleA</code> that takes ANSI encoded text.</p>
<p>Now, ANSI encoded text works fine if you only write English text, but as soon as you write text in other languages you might need to use special characters that are not possible to represent in <code>ANSI</code> but is possible in <code>utf-8</code> and our program will break.</p>
<p>That's why we'll convert our <code>utf-8</code> encoded text to <code>utf-16</code> encoded Unicode codepoints that can represent these characters and use the <code>WriteConsoleW</code> function.</p>
<pre><code class="language-rust no_run noplaypen">#[cfg(target_os = &quot;windows&quot;)]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {

    // let's convert our utf-8 to a format windows understands
    let msg: Vec&lt;u16&gt; = message.encode_utf16().collect();
    let msg_ptr = msg.as_ptr();
    let len = msg.len() as u32;

    let mut output: u32 = 0;
        let handle = unsafe { GetStdHandle(-11) };
        if handle  == -1 {
            return Err(io::Error::last_os_error())
        }

        let res = unsafe {
            WriteConsoleW(handle, msg_ptr, len, &amp;mut output, std::ptr::null())
            };

        if res  == 0 {
            return Err(io::Error::last_os_error());
        }

    assert_eq!(output, len);
    Ok(())
}
</code></pre>
<p>The first thing we do is to convert the text to utf-16 encoded text which
Windows uses. Fortunately, Rust has a built-in function to convert our <code>utf-8</code> encoded text to <code>utf-16</code> code points. <code>encode_utf16</code> returns an iterator over  <code>u16</code> code points that we can collect to a <code>Vec</code>.</p>
<pre><code class="language-rust no_run noplaypen">let msg: Vec&lt;u16&gt; = message.encode_utf16().collect();
let msg_ptr = msg.as_ptr();
let len = msg.len() as u32;
</code></pre>
<p>Next, we get the pointer to the underlying buffer of our <code>Vec</code> and get the
length.</p>
<pre><code class="language-rust no_run noplaypen">let handle = unsafe { GetStdHandle(-11) };
   if handle  == -1 {
       return Err(io::Error::last_os_error())
   }
</code></pre>
<p>The next is a call to <code>GetStdHandle</code>. We pass in the value <code>-11</code>. The values we
need to pass in for the different standard devices is actually documented
together with the <code>GetStdHandle</code> documentation:</p>
<table><thead><tr><th>Handle</th><th>Value</th></tr></thead><tbody>
<tr><td>Stdin</td><td>-10</td></tr>
<tr><td>Stdout</td><td>-11</td></tr>
<tr><td>StdErr</td><td>-12</td></tr>
</tbody></table>
<p>Now we're lucky here, it's not that common that we find this information
together with the documentation for the function we call, but it's very convenient when we do.</p>
<p>The return codes to expect is also documented thoroughly for all functions so we handle potential errors here in the same way as we did for the Linux/macOS syscalls.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let res = unsafe {
    WriteConsoleW(handle, msg_ptr, len, &amp;mut output, std::ptr::null())
    };

if res  == 0 {
    return Err(io::Error::last_os_error());
}
<span class="boring">}
</span></code></pre></pre>
<p>Next up is the call to the <code>WriteConsoleW</code> function. There is nothing too fancy about this.</p>
<h2><a class="header" href="#the-highest-level-of-abstraction" id="the-highest-level-of-abstraction">The highest level of abstraction</a></h2>
<p>This is simple, most standard libraries provide this abstraction for you. In rust that would simply be:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>println!(&quot;Hello world from Stdlib&quot;);
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#our-finished-cross-platform-syscall" id="our-finished-cross-platform-syscall">Our finished cross-platform syscall</a></h1>
<pre><pre class="playpen"><code class="language-rust">use std::io;

fn main() {
    let sys_message = String::from(&quot;Hello world from syscall!\n&quot;);
    syscall(sys_message).unwrap();
}

// and: http://man7.org/linux/man-pages/man2/write.2.html
#[cfg(not(target_os = &quot;windows&quot;))]
#[link(name = &quot;c&quot;)]
extern &quot;C&quot; {
    fn write(fd: u32, buf: *const u8, count: usize) -&gt; i32;
}

#[cfg(not(target_os = &quot;windows&quot;))]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    let res = unsafe { write(1, msg_ptr, len) };

    if res == -1 {
        return Err(io::Error::last_os_error());
    }
    Ok(())
}

#[cfg(target_os = &quot;windows&quot;)]
#[link(name = &quot;kernel32&quot;)]
extern &quot;stdcall&quot; {
    /// https://docs.microsoft.com/en-us/windows/console/getstdhandle
    fn GetStdHandle(nStdHandle: i32) -&gt; i32;
    /// https://docs.microsoft.com/en-us/windows/console/writeconsole
    fn WriteConsoleW(
        hConsoleOutput: i32,
        lpBuffer: *const u16,
        numberOfCharsToWrite: u32,
        lpNumberOfCharsWritten: *mut u32,
        lpReserved: *const std::ffi::c_void,
    ) -&gt; i32;
}

#[cfg(target_os = &quot;windows&quot;)]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {

    // let's convert our utf-8 to a format windows understands
    let msg: Vec&lt;u16&gt; = message.encode_utf16().collect();
    let msg_ptr = msg.as_ptr();
    let len = msg.len() as u32;

    let mut output: u32 = 0;
        let handle = unsafe { GetStdHandle(-11) };
        if handle  == -1 {
            return Err(io::Error::last_os_error())
        }

        let res = unsafe {
            WriteConsoleW(handle, msg_ptr, len, &amp;mut output, std::ptr::null())
            };

        if res  == 0 {
            return Err(io::Error::last_os_error());
        }

    assert_eq!(output, len);
    Ok(())
}
</code></pre></pre>
<h1><a class="header" href="#about-writing-cross-platform-abstractions" id="about-writing-cross-platform-abstractions">About writing cross-platform abstractions</a></h1>
<p>If you isolate the code needed only for Linux and macOS, you'll see that it's not many lines of code to write. But once you want to make a cross-platform variant, the amount of code explodes.</p>
<p>This is a recurring problem when we're curious about how this works on three different platforms, but we need some basic understanding of how the different operating systems work under the covers.</p>
<p>My experience, in general, is that Linux and macOS have simpler API requiring fewer lines of code, and often (but not always) the exact same call works for both systems.</p>
<p>Windows, on the other hand, is more complex, requires you to set up more structures to pass information (instead of using primitives), and often way more lines of code. What Windows does have is very good documentation so even though it's more work you'll also find the official documentation very helpful.</p>
<p>This complexity is why the Rust community (other languages often have something similar) gathers around crates like <a href="https://github.com/rust-lang/libc">libc</a> which already have defined most methods and constants you need.</p>
<h2><a class="header" href="#a-note-about-hidden-complexity" id="a-note-about-hidden-complexity">A note about &quot;hidden&quot; complexity</a></h2>
<p>There is a lot of &quot;hidden&quot; complexity when writing cross-platform code at this
level. One hurdle is to get something working, which can prove to be quite a
challenge. Getting it to work <strong>correctly</strong> and <strong>safely</strong> while covering all
edge cases is an additional challenge.</p>
<p><strong>Are we 100% sure that all valid <code>utf-8</code> code points which we use in Rust are valid <code>utf-16</code> encoded Unicode that Windows will display correctly?</strong></p>
<p>I think so, but being 100 % sure about <a href="https://en.wikipedia.org/wiki/Comparison_of_Unicode_encodings">this is not as easy as one might think</a>.</p>
<h1><a class="header" href="#does-the-cpu-cooperate-with-the-operating-system" id="does-the-cpu-cooperate-with-the-operating-system">Does the CPU cooperate with the Operating System?</a></h1>
<p>If you had asked me this question when I first thought I understood how programs work, I would most likely answer no. We run programs on the CPU and we can do whatever we want if we know how to do it. Now, first of all, I wouldn't have thought this through, but unless you learn how this work from the bottom up, it's not easy to know for sure.</p>
<p><strong>What started to make me think I was very wrong was some code looking like this:</strong></p>
<pre><pre class="playpen"><code class="language-rust">#![feature(llvm_asm)]
fn main() {
    let t = 100;
    let t_ptr: *const usize = &amp;t;
    let x = dereference(t_ptr);

    println!(&quot;{}&quot;, x);
}

fn dereference(ptr: *const usize) -&gt; usize {
    let res: usize;
    unsafe {
        llvm_asm!(&quot;mov ($1), $0&quot;:&quot;=r&quot;(res): &quot;r&quot;(ptr))
        };
    res
}
</code></pre></pre>
<blockquote>
<p>Here we write a <code>dereference</code> function using assembly instructions. We know there
is no way the OS is involved here.</p>
</blockquote>
<p>As you see, this code will output <code>100</code> as expected. But let's now instead create a pointer with the address <code>99999999999999</code> which we know is invalid and see what
happens when we pass that into the same function:</p>
<pre><pre class="playpen"><code class="language-rust">#![feature(llvm_asm)]
fn main() {
    let t = 99999999999999 as *const usize;
    let x = dereference(t);

    println!(&quot;{}&quot;, x);
}
<span class="boring">fn dereference(ptr: *const usize) -&gt; usize {
</span><span class="boring">    let res: usize;
</span><span class="boring">    unsafe {
</span><span class="boring">    llvm_asm!(&quot;mov ($1), $0&quot;:&quot;=r&quot;(res): &quot;r&quot;(ptr));
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    res
</span><span class="boring">}
</span></code></pre></pre>
<p>Now we get a segmentation fault. Not surprising really, but how does the CPU
know that we're not allowed to dereference this memory?</p>
<ul>
<li>Does the CPU ask the OS if this process is allowed to access this memory location every time we dereference something?</li>
<li>Won't that be very slow?</li>
<li>How does the CPU know that it has an OS running on top of it at all?</li>
<li>Do all CPUs know what a segmentation fault is?</li>
<li>Why do we get an error message at all and not just a crash?</li>
</ul>
<h2><a class="header" href="#down-the-rabbit-hole" id="down-the-rabbit-hole">Down the rabbit hole</a></h2>
<p>Yes, this is a small rabbit hole. It turns out that there
is a great deal of co-operation between the OS and the CPU, but maybe not the way you naively would think.</p>
<p>Many modern CPUs provide some basic infrastructure that Operating Systems use. This infrastructure gives us the security and stability we expect. Actually, most advanced CPUs provide a lot more options than operating systems like Linux, BSD and Windows actually use.</p>
<p>There is especially two that I want to address here:</p>
<ol>
<li>How the CPU prevents us from accessing memory we're not supposed to access</li>
<li>How the CPU handles asynchronous events like I/O</li>
</ol>
<p><em>We'll cover the first one here and the second in the next chapter.</em></p>
<blockquote>
<p>If you want to know more about how this works in detail I will absolutely
recommend that you give <a href="https://os.phil-opp.com/">Philipp Oppermann's excellent series</a>
a read. It's extremely well written and will answer all these questions and many more.</p>
</blockquote>
<h2><a class="header" href="#how-does-the-cpu-prevent-us-from-accessing-memory-were-not-supposed-to" id="how-does-the-cpu-prevent-us-from-accessing-memory-were-not-supposed-to">How does the CPU prevent us from accessing memory we're not supposed to?</a></h2>
<p>As I mentioned, modern CPUs have already some definition of basic concepts. Some examples of this are:</p>
<ul>
<li>Virtual memory</li>
<li>Page table</li>
<li>Page fault</li>
<li>Exceptions</li>
<li><a href="https://en.wikipedia.org/wiki/Protection_ring">Privilege level</a></li>
</ul>
<p>Exactly how this works will differ depending on the exact CPU so we'll treat them
in general terms here.</p>
<p>Most modern CPUs, however, have an MMU (Memory Management Unit). This is a part of the
CPU (often etched on the same dye even). The MMUs job is to translate between
the virtual address we use in our programs, to a physical address.</p>
<p>When the OS starts a process (like our program) it sets up a page table for our
process, and makes sure a special register on the CPU points to this page table.</p>
<p>Now, when we try to dereference <code>t_ptr</code> in the code above, the address is at some point
sent to the MMU for translation, which looks it up in the page table to translate
it to a physical address in memory where it can fetch the data.</p>
<p>In the first case, it will point to a memory address on our stack that holds the value <code>100</code>.</p>
<p>When we pass in <code>99999999999999</code> and ask it to fetch what's stored at that address
(which is what dereferencing does) it looks for the translation in the page table but can't find it.</p>
<p>The CPU then treats this as a <code>page fault</code>.</p>
<p>At boot, the OS provided the CPU with an Interrupt Descriptor Table. This table
has a predefined format where the OS provides handlers for the predefined
exceptions the CPU can encounter.</p>
<p>Since the OS provided a pointer to a function that handles <code>Page Fault</code> the CPU
jumps to that function when we try to dereference <code>99999999999999</code> and thereby hands over control to the Operating System.</p>
<p>The OS then prints a nice message for us letting us know that we encountered what it calls a <code>segmentation fault</code>. This message will therefore vary depending on the OS you run the code on.</p>
<h2><a class="header" href="#but-cant-we-just-change-the-page-table-in-the-cpu" id="but-cant-we-just-change-the-page-table-in-the-cpu">But can't we just change the page table in the CPU?</a></h2>
<p>Now, this is where <code>Privilege Level</code> comes in. Most modern operating systems operate with two <code>Ring Levels</code>. Ring 0, the kernel space, and Ring 3, user-space.</p>
<p><img src="./images/priv_rings.png" alt="Privilege rings" /></p>
<p>Most CPUs have a concept of more rings than what most modern operating systems use. This has historical reasons, which is also why <code>Ring 0</code> and <code>Ring 3</code> are used (and not 1, 2).</p>
<p>Now every entry in the page table has additional information about it, amongst that information is the information about what ring it belongs to. This information is set up when your OS boots up.</p>
<p>Code executed in <code>Ring 0</code> has almost unrestricted access to external devices, memory and is free to change registers that provide security at the hardware level.</p>
<p>Now, the code you write in <code>Ring 3</code> will typically have extremely restricted access to I/O and certain CPU registers (and instructions). Trying to issue an instruction or setting a register from <code>Ring 3</code> to change the <code>page table</code> will be prevented already at the CPU. The CPU will then treat this as an exception and jump to the handler for that exception provided by the OS.</p>
<p>This is also the reason why you have no other choice than to cooperate with the OS and handle I/O tasks through syscalls. The system wouldn't be very secure if this wasn't the case.</p>
<h1><a class="header" href="#interrupts-firmware-and-io" id="interrupts-firmware-and-io">Interrupts, Firmware and I/O</a></h1>
<p>We're nearing an end of the general CS subjects in the book, and we'll start
to dig our way out of the rabbit hole soon.</p>
<p>This part tries to tie things together and look at how the whole computer works
as a system to handle I/O and concurrency.</p>
<p>Let's get to it!</p>
<h2><a class="header" href="#a-simplified-overview" id="a-simplified-overview">A simplified overview</a></h2>
<p>Let's go through some of the steps where we imagine that we read from a
network card:</p>
<a href="./images/AsyncBasicsSimplified.png" target="_blank">
<p><img src="./images/AsyncBasicsSimplified.png" alt="Simplified Overview" /></p>
</a>
<p style="font-style: italic; text-align: center;">click the image to open a larger view</p>
<blockquote>
<p><strong>Disclaimer</strong>
We're making things simple here. This is a rather complex operation but we'll
focus on what interests us most and skip a few steps along the way.</p>
</blockquote>
<h2><a class="header" href="#1-our-code" id="1-our-code">1. Our code</a></h2>
<p>We register a socket. This happens by issuing a <code>syscall</code> to the OS. Depending
on the OS we either get a  <code>file descriptor</code> (macOS/Linux) or a <code>socket</code> (Windows).</p>
<p>The next step is that we register our interest in <code>read</code> events on that socket.</p>
<h2><a class="header" href="#2-registering-events-with-the-os" id="2-registering-events-with-the-os">2. Registering events with the OS</a></h2>
<p><strong>This is handled in one of three ways:</strong></p>
<ol type="A">
<li>
We tell the operating system that we're interested in `Read` events but we want
to wait for it to happen by `yielding` control over our thread to the OS. The OS
then suspends our thread by storing the register state and switches to some
other thread.
<p><strong>From our perspective this will be blocking our thread until we have data to read.</strong></p>
</li>
<li>
We tell the operating system that we're interested in `Read` events but we
just want a handle to a task which we can `poll` to check if the event is ready
or not.
<p><strong>The OS will not suspend our thread, so this will not block our code</strong></p>
</li>
<li>
We tell the operating system that we are probably going to be interested in
many events, but we want to subscribe to one event queue. When we `poll` this
queue it will block until one or more event occurs.
<p><strong>This will block our thread while we <code>wait</code> for events to occur</strong></p>
</li>
</ol>
<blockquote>
<p>My next book will be about alternative C since that is a very interesting
model of handling I/O events that's going to be important later on to understand
why Rust's concurrency abstractions are modeled the way they are. For that reason
we won't cover this in detail here.</p>
</blockquote>
<h2><a class="header" href="#3-the-network-card" id="3-the-network-card">3. The Network Card</a></h2>
<blockquote>
<p>We're skipping some steps here but it's not very vital to our understanding.</p>
</blockquote>
<p>Meanwhile on the network card there is a small microcontroller running
specialized firmware. We can imagine that this microcontroller is polling in a
busy loop checking if any data is incoming.</p>
<blockquote>
<p>The exact way the Network Card handles its internals can be different from this
(and most likely is). The important part is that there is a very simple but specialized CPU running
on the network card doing <strong>work</strong> to check if there are incoming events.</p>
</blockquote>
<p>Once the firmware registers incoming data, it issues a Hardware Interrupt.</p>
<h2><a class="header" href="#4-hardware-interrupt" id="4-hardware-interrupt">4. Hardware Interrupt</a></h2>
<blockquote>
<p>This is a very simplified explanation. If you're interested in knowing more
about how this works, I can recommend Robert Mustacchi's excellent article
<a href="https://www.joyent.com/blog/virtualizing-nics">Turtles on the wire: understanding how the OS uses the modern NIC</a>.</p>
</blockquote>
<p>Modern CPUs have a set of <code>Interrupt Request Lines</code> for it to handle events that occur from
external devices. A CPU has a fixed set of interrupt lines.</p>
<p>A hardware interrupt is an electrical signal that can occur at <em>any time</em>. The
CPU immediately <strong>interrupts</strong> its normal workflow to handle the interrupt by
saving the state of its registers and looking up the interrupt handler. The interrupt handlers are defined in the Interrupt Descriptor Table.</p>
<h2><a class="header" href="#5-interrupt-handler" id="5-interrupt-handler">5. Interrupt Handler</a></h2>
<p>The <a href="https://en.wikipedia.org/wiki/Interrupt_descriptor_table">Interrupt Descriptor Table (IDT)</a> is a table where the OS (or a driver) registers handlers for different interrupts that may occur. Each entry points to a handler function for a specific interrupt. The handler function for a Network Card would typically be registered and handled by a <code>driver</code> for that card.</p>
<blockquote>
<p>The IDT is not stored on the CPU as it might seem in the diagram. It's located
in a fixed and know location in main memory. The CPU only holds a pointer to the
table in one of it's registers.</p>
</blockquote>
<h2><a class="header" href="#6-writing-the-data" id="6-writing-the-data">6. Writing the data</a></h2>
<p>This is a step that might vary a lot depending on the CPU and the firmware on the
network card. If the Network Card and the CPU supports <a href="https://en.wikipedia.org/wiki/Direct_memory_access">Direct Memory Access</a> (which should be the standard on all modern systems today) the Network Card will write data directly to a set of buffers the OS already has set up in main memory.</p>
<p>In such a system the <code>firmware</code> on the Network Card might issue an <code>Interrupt</code> when the data is <strong>written</strong> to memory. <code>DMA</code> is very efficient
since the CPU is only notified when the data is already in memory. On older systems the
CPU needed to devote resources to handle the data transfer from the
network card.</p>
<p><em>The DMAC (Direct Memory Access Controller) is just added since in such a system,
it would control the access to memory. It's not part of the CPU as in the
diagram above. We're deep enough in the rabbit hole now and this is not really important for us right now so let's move on.</em></p>
<h2><a class="header" href="#7-the-driver" id="7-the-driver">7. The driver</a></h2>
<p>The <code>driver</code> would normally handle the communication between the OS and the Network Card.
At <em>some point</em> the buffers are filled, and the network card issues an <code>Interrupt</code>. The CPU then jumps to the handler of that interrupt. The interrupt handler for this exact type
of interrupt is registered by the driver, so it's actually the driver that handles this event and in turn informs the kernel that the data is ready to be read.</p>
<h2><a class="header" href="#8-reading-the-data" id="8-reading-the-data">8. Reading the data</a></h2>
<p>Depending on whether we chose alternative A, B or C the OS will:</p>
<ol>
<li>Wake our thread</li>
<li>Return <code>Ready</code> on the next <code>poll</code></li>
<li>Wake the thread and return a <code>Read</code> event for the handler we registered.</li>
</ol>
<h2><a class="header" href="#interrupts" id="interrupts">Interrupts</a></h2>
<p>As I hinted at above, there are two kinds of interrupts:</p>
<ol>
<li>Hardware Interrupts</li>
<li>Software Interrupts</li>
</ol>
<p>They are very different in nature.</p>
<h3><a class="header" href="#hardware-interrupts" id="hardware-interrupts">Hardware Interrupts</a></h3>
<p>Hardware interrupts are created by sending an electrical signal through an <a href="https://en.wikipedia.org/wiki/Interrupt_request_(PC_architecture)#x86_IRQs">Interrupt Request Line (IRQ)</a>. These hardware lines signals the CPU directly.</p>
<h3><a class="header" href="#software-interrupts" id="software-interrupts">Software Interrupts</a></h3>
<p>These are interrupts issued from software instead of hardware. As in the case of a hardware interrupt, the CPU jumps to the Interrupt Descriptor Table and runs the handler for the specified interrupt.</p>
<h3><a class="header" href="#firmware" id="firmware">Firmware</a></h3>
<p>Firmware doesn't get much attention from most of us; however, they're a crucial part of the world we live in. They run on all kinds of hardware, and have all kinds of strange and peculiar ways to make the computer we program on work.</p>
<p>When I think about firmware, I think about the scenes from Star Wars where they walk into a bar with all kinds of strange and obscure creatures. I imagine the world of firmware is much like this, few of us know what they do or how they work on a particular system.</p>
<p>Now, firmware needs a microcontroller or similar to be able to work. Even the CPU has firmware which makes it work. That means there are many more small &quot;CPUs&quot; on our system than the cores we program against.</p>
<p>Why is this important? Well, you remember that concurrency is all about efficiency right? Well, since we have many CPU's already doing work for us on our system, one of our concerns is to not replicate or duplicate that work when we write code.</p>
<p>If a network card has firmware that continually checks if new data has arrived, it's pretty wasteful if we duplicate that by letting our CPU continually check if new data arrives as well. It's much better if we either check once in a while or even better, gets notified when data has arrived for us.</p>
<h1><a class="header" href="#strategies-for-handling-io" id="strategies-for-handling-io">Strategies for handling I/O</a></h1>
<p>Before we dive into Writing some code we'll finish off this part of the book talking a bit about different strategies of handling I/O and concurrency. Now, just note that I'm covering I/O in general here, but I use network communication as the main example. Different strategies can have different strengths depending on what type of I/O we're talking about.</p>
<h2><a class="header" href="#1-using-os-threads" id="1-using-os-threads">1. Using OS threads</a></h2>
<p>Now one way of accomplishing this is letting the OS take care of everything for us. We do this by simply spawning a new OS thread for each task we want to accomplish and write code like we normally would.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Simple</li>
<li>Easy to code</li>
<li>Reasonably performant</li>
<li>You get parallelism for free</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>OS level threads come with a rather large stack. If you have many tasks waiting simultaneously (like you would in a web-server under heavy load) you'll run out of memory pretty soon.</li>
<li>There are a lot of syscalls involved. This can be pretty costly when the number of tasks is high.</li>
<li>The OS has many things it needs to handle. It might not switch back to your thread as fast as you'd wish</li>
<li>The OS doesn't know which tasks to prioritize, and you might want to give some tasks a higher priority than others.</li>
</ul>
<h2><a class="header" href="#2-green-threads" id="2-green-threads">2. Green threads</a></h2>
<p>Another common way of handling this is green threads. Languages like Go uses this to great success. In many ways this is similar to what the OS does but the runtime can be better adjusted and suited to your specific needs.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Simple to use for the user. The code will look like it does when using OS threads</li>
<li>Reasonably performant</li>
<li>Abundant memory usage is less of a problem</li>
<li>You are in full control over how threads are scheduled and if you want you can prioritize them differently.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>You need a runtime, and by having that you are duplicating part of the work the OS already does. The runtime will have a cost which in some cases can be substantial.</li>
<li>Can be difficult to implement in a flexible way to handle a wide variety of tasks</li>
</ul>
<h2><a class="header" href="#3-poll-based-event-loops-supported-by-the-os" id="3-poll-based-event-loops-supported-by-the-os">3. Poll based event loops supported by the OS</a></h2>
<p>The third way we're covering today is the one that most closely matches an ideal solution. In this solution we register an interest in an event, and then let the OS tell us when it's ready.</p>
<p>The way this works is that we tell the OS that we're interested in knowing when data is arriving for us on the network card. The network card issues an interrupt when something has happened, in which case the driver lets the OS know that the data is ready.</p>
<p>Now, we still need a way to &quot;suspend&quot; many tasks while waiting, and this is where Node's &quot;runtime&quot; or Rust's Futures come in to play.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Close to optimal resource utilization</li>
<li>It's very efficient</li>
<li>Gives us the maximum amount of flexibility to decide how to handle the events that occurs</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Different operating systems have different ways of handling these kind of queues. Some of them are difficult to reconcile with each other. Some operating systems have limitations on what I/O operations support this method.</li>
<li>Great flexibility comes with a good deal of complexity</li>
<li>Difficult to write an ergonomic API with an abstraction layer that accounts for the differences between the operating systems without introducing unwanted costs.</li>
<li>Only solves part of the problem—the programmer still needs a strategy for suspending tasks that are waiting.</li>
</ul>
<h2><a class="header" href="#final-note" id="final-note">Final note</a></h2>
<p>The Node runtime uses a combination of both 1 and 3, but it tries to force all I/O to use alternative 3. This design is also part of the reason why Node is so good at handling many connections concurrently.  Node uses a callback-based approach to suspend tasks.</p>
<p>Rust's async story is modeled around option 3, and one of the reasons it has taken a long time is related to the <em>cons</em> of this method and choosing a way to model how tasks should be suspended. Rust's Futures model a task as a <a href="https://en.wikipedia.org/wiki/Finite-state_machine">State Machine</a> where a suspension point represents a <code>state</code>.</p>
<h1><a class="header" href="#epoll-kqueue-and-iocp" id="epoll-kqueue-and-iocp">Epoll, Kqueue and IOCP</a></h1>
<p>There are some well-known libraries which implement a cross platform event queue using Epoll, Kqueue and IOCP for Linux, Mac, and Windows, respectively.</p>
<p>Part of Node's runtime is based on <a href="https://github.com/libuv/libuv">libuv</a>, which is a cross platform
asynchronous I/O library. <code>libuv</code> is not only used in Node but also forms the foundation
of how <a href="https://julialang.org/">Julia</a> and <a href="https://github.com/saghul/pyuv">Pyuv</a> create a cross platform event queue; most
languages have bindings for it.</p>
<p>In Rust we have <a href="https://github.com/tokio-rs/mio">mio - Metal IO</a>. <code>Mio</code> powers the OS event queue used in <a href="https://github.com/tokio-rs/tokio">Tokio</a>, which is a runtime that provides I/O, networking, scheduling etc. <code>Mio</code> is to <code>Tokio</code> what <code>libuv</code> is to <code>Node</code>.</p>
<p><code>Tokio</code> powers many web frameworks, among those is <a href="https://github.com/actix/actix-web">Actix Web</a>, which is known to be very performant.</p>
<p>Since we want
to understand how everything works, I decided to create an extremely
simplified version of an event queue. I called it <code>minimio</code> since it's greatly inspired by <code>mio</code>.</p>
<blockquote>
<p>I have written about how this works in detail in <a href="https://cfsamsonbooks.gitbook.io/epoll-kqueue-iocp-explained/">Epoll, Kqueue and IOCP explained</a>.
In that book we also create the event loop which we will use as the cross platform event loop in this book. You can visit the code at its <a href="https://github.com/cfsamson/examples-minimio">Github repository if you're
curious</a>.</p>
</blockquote>
<p>Nevertheless, we'll give each of them a brief introduction here so you know the basics.</p>
<h2><a class="header" href="#why-use-an-os-backed-event-queue" id="why-use-an-os-backed-event-queue">Why use an OS-backed event queue?</a></h2>
<p>If you remember my previous chapters, you know that we need to cooperate closely
with the OS to make I/O operations as efficient as possible. Operating systems like
Linux, macOS and Windows provide several ways of performing I/O, both blocking and
non-blocking.</p>
<p>So blocking operations are the least flexible to use for us as programmers since we yield control to the OS, which suspends our thread. The big advantage is that our thread gets woken up once the event we're waiting for is ready.</p>
<p>Non-blocking methods are more flexible but need to have a way to tell us if a task is ready or not. This is most often done by returning some kind of data that says if it's <code>Ready</code> or <code>NotReady</code>. One drawback is that we need to check this status regularly to be able to tell if the state has changed.</p>
<p>Event queuing via Epoll/kqueue/IOCP is a way to enjoy the flexibility of a non-blocking method without its aforementioned drawback.</p>
<blockquote>
<p>We will not cover methods like <code>poll</code> and <code>select</code>, but I have an <a href="http://web.archive.org/web/20190112082733/https://people.eecs.berkeley.edu/%7Esangjin/2012/12/21/epoll-vs-kqueue.html">article for you
here</a>
if you want to learn a bit about these methods and how they differ from <code>epoll</code>.</p>
</blockquote>
<h2><a class="header" href="#readiness-based-event-queues" id="readiness-based-event-queues">Readiness-based event queues</a></h2>
<p>Epoll and Kqueue are known as readiness-based event queues, which means they let you know when an action is ready to be performed.  An example of this is a socket that is ready to be read from.</p>
<p><strong>Basically this happens when we want to read data from a socket using epoll/kqueue:</strong></p>
<ol>
<li>We create an event queue by calling the syscall <code>epoll_create</code> or <code>kqueue</code>.</li>
<li>We ask the OS for a file descriptor representing a network socket.</li>
<li>Through another syscall, we register an interest in <code>Read</code> events on this socket. It's important that we also inform the OS that we'll be expecting to receive a notification when the event is ready in the event queue we created in (1).</li>
<li>Next, we call <code>epoll_wait</code> or <code>kevent</code> to wait for an event. This will block (suspend) the thread it's called on.</li>
<li>When the event is ready, our thread is unblocked (resumed), and we return from our &quot;wait&quot; call with data about the event that occurred.</li>
<li>We call <code>read</code> on the socket we created in 2.</li>
</ol>
<h2><a class="header" href="#completion-based-event-queues" id="completion-based-event-queues">Completion-based event queues</a></h2>
<p>IOCP stands for I/O Completion Ports and is a completion-based event queue. This type of queue notifies you when events are completed.  An example of this is when data has been read into a buffer.</p>
<p>Below is a basic breakdown of what happens in this type of event queue:</p>
<ol>
<li>We create an event queue by calling the syscall <code>CreateIoCompletionPort</code>.</li>
<li>We create a buffer and ask the OS to give us a handle to a socket.</li>
<li>We register an interest in <code>Read</code> events on this socket with another syscall,
but this time we also pass in the buffer we created in (2) to which the data will
be read.</li>
<li>Next, we call <code>GetQueuedCompletionStatusEx</code>, which will block until an event has
completed.</li>
<li>Our thread is unblocked, and our buffer is now filled with the data we're interested in.</li>
</ol>
<h2><a class="header" href="#epoll" id="epoll">Epoll</a></h2>
<p><code>Epoll</code> is the Linux way of implementing an event queue. In terms of functionality, it has a lot in common with <code>Kqueue</code>. The advantage of using <code>epoll</code> over other similar methods on Linux like <code>select</code> or <code>poll</code> is that <code>epoll</code> was designed to work very efficiently with a large number of events.</p>
<h3><a class="header" href="#kqueue" id="kqueue">Kqueue</a></h3>
<p><code>Kqueue</code> is the macOS way of implementing an event queue, which originated from BSD, in operating systems such as FreeBSD, OpenBSD, etc. In terms of high level functionality,
it's similar to <code>Epoll</code> in concept but different in actual use.</p>
<p>Some argue it's a bit more complex to use and a bit more abstract and &quot;general&quot;.</p>
<h3><a class="header" href="#iocp" id="iocp">IOCP</a></h3>
<p><code>IOCP</code> or Input Output Completion Ports is the way Windows handles this type of event queue.</p>
<p>A <code>Completion Port</code> will let you know when an event has <code>Completed</code>. Now this might
sound like a minor difference, but it's not. This is especially apparent when you want to write a library since abstracting over both means you'll either have to model <code>IOCP</code> as <code>readiness-based</code> or model <code>epoll/kqueue</code> as completion-based.</p>
<p>Lending out a buffer to the OS also provides some challenges since it's very
important that this buffer stays untouched while waiting for an operation to
return.</p>
<blockquote>
<p>My experience investigating this suggests that getting <code>readiness-based</code>
models to behave like the <code>completion-based</code> models is easier than the other
way around. This means you should get IOCP to work first and then fit <code>epoll</code> or <code>kqueue</code>
into that design.</p>
</blockquote>
<h1><a class="header" href="#introducing-our-main-example" id="introducing-our-main-example">Introducing our main example</a></h1>
<p>Now we've finally come to the part of this book where we will write some code.</p>
<p>The Node event loop is a complex piece of software developed over many years. We
will have to simplify things a lot.</p>
<p>We'll try to implement the parts that are important for us to understand Node a
little better and most importantly use it as an example where we can use our
knowledge from the previous chapters to make something that actually works.</p>
<p>Our main goal here is to explore async concepts.  Using Node as an example is
mostly for fun.</p>
<p><strong>We want to write something like this:</strong></p>
<pre><pre class="playpen"><code class="language-rust no_run">/// Think of this function as the javascript program you have written
fn javascript() {
    print(&quot;First call to read test.txt&quot;);
    Fs::read(&quot;test.txt&quot;, |result| {
        let text = result.into_string().unwrap();
        let len = text.len();
        print(format!(&quot;First count: {} characters.&quot;, len));

        print(r#&quot;I want to create a &quot;magic&quot; number based on the text.&quot;#);
        Crypto::encrypt(text.len(), |result| {
            let n = result.into_int().unwrap();
            print(format!(r#&quot;&quot;Encrypted&quot; number is: {}&quot;#, n));
        })
    });

    print(&quot;Registering immediate timeout 1&quot;);
    set_timeout(0, |_res| {
        print(&quot;Immediate1 timed out&quot;);
    });
    print(&quot;Registering immediate timeout 2&quot;);
    set_timeout(0, |_res| {
        print(&quot;Immediate2 timed out&quot;);
    });

    // let's read the file again and display the text
    print(&quot;Second call to read test.txt&quot;);
    Fs::read(&quot;test.txt&quot;, |result| {
        let text = result.into_string().unwrap();
        let len = text.len();
        print(format!(&quot;Second count: {} characters.&quot;, len));

        // aaand one more time but not in parallel.
        print(&quot;Third call to read test.txt&quot;);
        Fs::read(&quot;test.txt&quot;, |result| {
            let text = result.into_string().unwrap();
            print_content(&amp;text, &quot;file read&quot;);
        });
    });

    print(&quot;Registering a 3000 and a 500 ms timeout&quot;);
    set_timeout(3000, |_res| {
        print(&quot;3000ms timer timed out&quot;);
        set_timeout(500, |_res| {
            print(&quot;500ms timer(nested) timed out&quot;);
        });
    });

    print(&quot;Registering a 1000 ms timeout&quot;);
    set_timeout(1000, |_res| {
        print(&quot;SETTIMEOUT&quot;);
    });

    // `http_get_slow` lets us define a latency we want to simulate
    print(&quot;Registering http get request to google.com&quot;);
    Http::http_get_slow(&quot;http//www.google.com&quot;, 2000, |result| {
        let result = result.into_string().unwrap();
        print_content(result.trim(), &quot;web call&quot;);
    });
}

fn main() {
    let rt = Runtime::new();
    rt.run(javascript);
}
</code></pre></pre>
<blockquote>
<p>We'll have print statements at strategic places in our code so we can get a view of what's actually happening when and where.</p>
</blockquote>
<p>Right off the bat you'll see that something is strange with the Rust code we
have written in our example.</p>
<p>The code uses callbacks when we have an async operation we want to execute just
like Javascript does, and we have &quot;magic&quot; modules like <code>Fs</code> or <code>Crypto</code> that
we can call, just like when you import modules from Node.</p>
<p>Our code here is mostly calling functions that register an event and stores a
callback to be run when the event is ready.</p>
<p><strong>An example of this is the <code>set_timeout</code> function:</strong></p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>set_timeout(0, |_res| {
    print(&quot;Immediate1 timed out&quot;);
});
<span class="boring">}
</span></code></pre></pre>
<p>What we really do here is register interest in a <code>timeout</code> event, and when that event occurs we want to run
the callback <code>|_res| { print(&quot;Immediate1 timed out&quot;); }</code>.</p>
<p>Now the parameter <code>_res</code> is
an argument that is passed into our callback. In javascript it would be left out, but since we use a typed language we have created a type called <code>Js</code>.</p>
<p><code>Js</code> is an enum that represents Javascript types. In the case of <code>set_timeout</code> it's <code>Js::undefined</code>. In the case of <code>Fs::read</code> it's an <code>Js::String</code> and so on.</p>
<p>When we run this code we'll get something looking like this:</p>
<video autoplay loop>
<source src="./images/example_run.mp4" type="video/mp4">
Can't display video.
</video>
<p>And here is what our output will look like:</p>
<pre><code>Thread: main     First call to read test.txt
Thread: main     Registering immediate timeout 1
Thread: main     Registered timer event id: 2
Thread: main     Registering immediate timeout 2
Thread: main     Registered timer event id: 3
Thread: main     Second call to read test.txt
Thread: main     Registering a 3000 and a 500 ms timeout
Thread: main     Registered timer event id: 5
Thread: main     Registering a 1000 ms timeout
Thread: main     Registered timer event id: 6
Thread: main     Registering http get request to google.com
Thread: pool3    received a task of type: File read
Thread: pool2    received a task of type: File read
Thread: main     Event with id: 7 registered.
Thread: main     ===== TICK 1 =====
Thread: main     Immediate1 timed out
Thread: main     Immediate2 timed out
Thread: pool3    finished running a task of type: File read.
Thread: pool2    finished running a task of type: File read.
Thread: main     First count: 39 characters.
Thread: main     I want to create a &quot;magic&quot; number based on the text.
Thread: pool3    received a task of type: Encrypt
Thread: main     ===== TICK 2 =====
Thread: main     SETTIMEOUT
Thread: main     Second count: 39 characters.
Thread: main     Third call to read test.txt
Thread: main     ===== TICK 3 =====
Thread: pool2    received a task of type: File read
Thread: pool3    finished running a task of type: Encrypt.
Thread: main     &quot;Encrypted&quot; number is: 63245986
Thread: main     ===== TICK 4 =====
Thread: pool2    finished running a task of type: File read.

===== THREAD main START CONTENT - FILE READ =====
Hello world! This is text to encrypt!
... [Note: Abbreviated for display] ...
===== END CONTENT =====

Thread: main     ===== TICK 5 =====
Thread: epoll    epoll event 7 is ready

===== THREAD main START CONTENT - WEB CALL =====
HTTP/1.1 302 Found
Server: Cowboy
Location: http://http/www.google.com
... [Note: Abbreviated for display] ...
===== END CONTENT =====

Thread: main     ===== TICK 6 =====
Thread: epoll    epoll event timeout is ready
Thread: main     ===== TICK 7 =====
Thread: main     3000ms timer timed out
Thread: main     Registered timer event id: 10
Thread: epoll    epoll event timeout is ready
Thread: main     ===== TICK 8 =====
Thread: main     500ms timer(nested) timed out
Thread: pool0    received a task of type: Close
Thread: pool1    received a task of type: Close
Thread: pool2    received a task of type: Close
Thread: pool3    received a task of type: Close
Thread: epoll    received event of type: Close
Thread: main     FINISHED
</code></pre>
<p>Don't worry, we'll explain everything, but I just wanted to start off by
explaining where we want to end up.</p>
<h1><a class="header" href="#what-is-node" id="what-is-node">What is Node?</a></h1>
<p>We have to start with a short explanation of what Node is, just so we're on the same page.</p>
<p>Node is a Javascript runtime allowing Javascript to run on your desktop (or server). Javascript was originally designed as a scripting language for the browser, which means that it relies on the browser to both interpret it and provide a runtime for it.</p>
<p>This also means that Javascript on the desktop needs to be both interpreted (or compiled) and provided with a runtime to be able to do anything meaningful. On the desktop, the <a href="https://en.wikipedia.org/wiki/V8_JavaScript_engine">V8 javascript engine</a> compiles Javascript, and <a href="https://en.wikipedia.org/wiki/Node.js">Node</a> provides the runtime.</p>
<p>Javascript has one advantage from a language design perspective: Everything is designed to be handled asynchronously. And as you know by now, this is pretty crucial if we want to make the most out of our hardware, especially if you have a lot of I/O operations to take care of.</p>
<p>One such scenario is a Web server. Web servers handle a lot of I/O tasks whether it's reading from the file system or communicating via the network card.</p>
<h2><a class="header" href="#why-node-in-particular" id="why-node-in-particular">Why Node In Particular?</a></h2>
<ul>
<li>Javascript is unavoidable when doing web development for the browser. Using Javascript on the server allows programmers to use the same language for both front-end and back-end development.</li>
<li>There is a potential for code reuse between the back-end and the front-end</li>
<li>The design of Node allows it to make very performant web servers</li>
<li>Working with Json and web APIs is very easy when you only deal with Javascript</li>
</ul>
<h2><a class="header" href="#helpful-facts" id="helpful-facts">Helpful Facts</a></h2>
<p>Let's start off by debunking some myths that might make it easier to follow along when we start to code.</p>
<h3><a class="header" href="#the-javascript-event-loop" id="the-javascript-event-loop">The Javascript Event Loop</a></h3>
<p>Javascript is a scripting language and can't do much on its own. It doesn't have an event loop. Now in a web browser, the browser provides a runtime, which includes an event loop. And on the server, Node provides this functionality.</p>
<p>You might say that Javascript as a language would be difficult to run (due to its callback-based model) without some sort of event loop, but that's beside the point.</p>
<h3><a class="header" href="#node-is-multithreaded" id="node-is-multithreaded">Node is Multithreaded</a></h3>
<p>Contrary to what I've seen claimed on several occasions, Node uses a thread pool so it's multithreaded. However, the part of Node that &quot;progresses&quot; your code, does indeed run on a single thread. When we say &quot;don't block the event loop&quot; we're referring to this thread since that will prevent Node from making progress on other tasks.</p>
<p>We'll see exactly why blocking this thread is a problem and how that's handled.</p>
<h3><a class="header" href="#the-v8-javascript-engine" id="the-v8-javascript-engine">The V8 Javascript Engine</a></h3>
<p>Now, this is where we need to focus a bit. The V8 engine is a javascript JIT compiler. That means that when you write a <code>for</code> loop, the V8 engine translates this to instructions that run on your CPU. There are many javascript engines, but Node was originally implemented on top of the V8 engine.</p>
<p>The V8 engine itself isn't very useful for us; it just interprets our Javascript. It can't do I/O, set up a runtime or anything like that. Writing Javascript only with V8 will be a very limited experience.</p>
<blockquote>
<p>Since we write Rust (even though we made it look a bit &quot;javascripty&quot;), we'll not cover the part of interpreting javascript. We'll just focus on how Node works and how it handles concurrency since that's our main focus right now.</p>
</blockquote>
<h3><a class="header" href="#nodes-event-loops" id="nodes-event-loops">Nodes Event Loop(s)</a></h3>
<p>Node internally divides its real work into two categories:</p>
<h4><a class="header" href="#io-bound-tasks" id="io-bound-tasks">I/O-bound tasks</a></h4>
<p>Tasks that mainly wait for some external event to occur are handled by the cross platform epoll/kqueue/IOCP event queue implemented in <code>libuv</code> and in our case <code>minimio</code>.</p>
<h4><a class="header" href="#cpu-bound-tasks" id="cpu-bound-tasks">CPU-bound tasks</a></h4>
<p>Tasks that are predominately CPU intensive are handled by a thread pool. The default size of this thread pool is 4 threads, but that can be configured by the Node runtime.</p>
<p>I/O tasks which can't be handled by the cross platform event queue are also handled here, which is the case with file reads that we use in our example.</p>
<p>Most C++ extensions for Node use this thread pool to perform their work, and that is one of many reasons they are used for calculation-intensive tasks.</p>
<h2><a class="header" href="#further-information" id="further-information">Further Information</a></h2>
<p>If you do want to know more about the Node event loop, I have one short page of the <code>libuv</code> documentation I can
refer you to and two talks for you that I find great (and correct) on this subject:</p>
<p><a href="http://docs.libuv.org/en/v1.x/design.html#design-overview">Libuv Design Overview</a></p>
<p>This first talk one is held by <a href="https://github.com/piscisaureus">@piscisaureus</a> and is an excellent 15 minute overview. I especially recommend this one as its short and to the point.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PNa9OMajw9w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>The second one is slightly longer but is also an excellent talk held by <a href="https://github.com/nebrius">Bryan Hughes</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/zphcsoSJMvM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>Now, relax, get a cup of tea and sit back while we go through everything together.</p>
<h1><a class="header" href="#whats-our-plan" id="whats-our-plan">What's our plan?</a></h1>
<p>I'll briefly list what we need to do to get this working here:</p>
<h2><a class="header" href="#we-need-two-event-queues" id="we-need-two-event-queues">We need two event queues:</a></h2>
<ol>
<li>We need a thread pool to execute our CPU intensive tasks or tasks that we want
too run asynchronously but not in our OS backed event queue</li>
<li>We need to make a simple cross platform <code>epoll/kqueue/IOCP</code> event loop. Now
this turns out to be extremely interesting, but it's also a lot of code, so I split
this section off into a separate &quot;companion book&quot; for those that want to explore this further.
We use this library here called <code>minimio</code>.</li>
</ol>
<blockquote>
<p>Granted, we're breaking our rule that we don't use any dependencies, but it's our own dependency which we'll explain fully in due time.</p>
</blockquote>
<h2><a class="header" href="#we-need-a-runtime" id="we-need-a-runtime">We need a runtime</a></h2>
<p><strong>Our runtime will:</strong></p>
<ol>
<li>Store our callbacks to be run at a later point</li>
<li>Send tasks to be executed on the thread pool</li>
<li>Register interests with the OS (through <code>minimio</code>)</li>
<li>Poll our two event sources for new events</li>
<li>Handle timers</li>
<li>Provide a way for &quot;modules&quot; like <code>Fs</code> and <code>Crypto</code> to register tasks</li>
<li>Progress all our tasks until we're finished</li>
</ol>
<h2><a class="header" href="#we-need-a-few-modules" id="we-need-a-few-modules">We need a few modules</a></h2>
<ol>
<li>For handling file system tasks <code>Fs</code></li>
<li>For handling http calls <code>Http</code></li>
<li>For handling cryptological tasks <code>Crypto</code></li>
</ol>
<h2><a class="header" href="#we-need-to-make-some-helpers" id="we-need-to-make-some-helpers">We need to make some helpers</a></h2>
<p>We need some helpers to make our code readable and to provide the output we want
to see. In contrast to any real runtime, we're interested in knowing what happens
and when. To help with that we define three extra methods:</p>
<p><code>print</code> which prints out a message that first tells us what thread the message is being outputted from and then a message we provide.</p>
<p><code>print_content</code> does the same as <code>print</code> but is a way for us to print out more than a message in a nice way.</p>
<p><code>current</code> is just a shortcut for us to get the name of the current thread. Since we want to track what's happening, we're going to need to print out which thread is issuing what output, so this will avoid cluttering up our code too much along the way.</p>
<h2><a class="header" href="#minimio" id="minimio">Minimio</a></h2>
<p>Minimio is a cross platform epoll/kqueue/IOCP based event loop that we will cover in the next book. I originally included it here, but implementing that for three architectures turned out to be a bit more involved than I first thought and needed more space than would fit in this book.</p>
<p>It's also easier to just read up on epoll/kqueue/IOCP when it's concentrated in a separate book.</p>
<h1><a class="header" href="#implementing-our-own-runtime" id="implementing-our-own-runtime">Implementing our own runtime</a></h1>
<p>Let's start to get some code written down; we have a lot to do.</p>
<p>The way we'll go about this is that I'll go through everything the way I find it easiest to parse and understand. That also means that sometimes I have to introduce a bit of code that will be explained later.  Try not to worry if you don't understand something at first.  I'll be going through everything.</p>
<p>The very first thing we need to do is to create a Rust project to run our code in:</p>
<pre><code>cargo new async-basics
cd async-basics
</code></pre>
<p>Now, as I've explained, we'll need to use the <code>minimio</code> library (which will be explained in a separate book, but you can already look through the source code if you want to):</p>
<p>In <code>Cargo.toml</code></p>
<pre><code class="language-toml">[dependencies]
minimio = {git = &quot;https://github.com/cfsamson/examples-minimio&quot;, branch = &quot;master&quot;}
</code></pre>
<p>A second option is to clone the repository containing all the code we're going
to write and go through that as we go along:</p>
<pre><code>git clone https://github.com/cfsamson/examples-node-eventloop
</code></pre>
<p>The next thing we need is a <code>Runtime</code> to hold all the state our <code>Runtime</code> needs.</p>
<p>First navigate to <code>main.rs</code> (located in <code>src/main.rs</code>).</p>
<blockquote>
<p>We'll write everything in one file this time in roughly the same order as we
go through them in this book.</p>
</blockquote>
<h2><a class="header" href="#runtime-struct" id="runtime-struct">Runtime struct</a></h2>
<p>I've added comments to the code so it's easier to remember and understand.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub struct Runtime {
    /// Available threads for the threadpool
    available_threads: Vec&lt;usize&gt;,
    /// Callbacks scheduled to run
    callbacks_to_run: Vec&lt;(usize, Js)&gt;,
    /// All registered callbacks
    callback_queue: HashMap&lt;usize, Box&lt;dyn FnOnce(Js)&gt;&gt;,
    /// Number of pending epoll events, only used by us to print for this example
    epoll_pending_events: usize,
    /// Our event registrator which registers interest in events with the OS
    epoll_registrator: minimio::Registrator,
    // The handle to our epoll thread
    epoll_thread: thread::JoinHandle&lt;()&gt;,
    /// None = infinite, Some(n) = timeout in n ms, Some(0) = immediate
    epoll_timeout: Arc&lt;Mutex&lt;Option&lt;i32&gt;&gt;&gt;,
    /// Channel used by both our threadpool and our epoll thread to send events
    /// to the main loop
    event_reciever: Receiver&lt;PollEvent&gt;,
    /// Creates an unique identity for our callbacks
    identity_token: usize,
    /// The number of events pending. When this is zero, we're done
    pending_events: usize,
    /// Handles to our threads in the threadpool
    thread_pool: Vec&lt;NodeThread&gt;,
    /// Holds all our timers, and an Id for the callback to run once they expire
    timers: BTreeMap&lt;Instant, usize&gt;,
    /// A struct to temporarely hold timers to remove. We let Runtinme have
    /// ownership so we can reuse the same memory
    timers_to_remove: Vec&lt;Instant&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p>Now, I've added some comments here to explain what they're for and in the coming
chapters we'll cover every one of them.</p>
<p>I'll continue by defining some of the types we use here.</p>
<h2><a class="header" href="#task-1" id="task-1">Task</a></h2>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct Task {
    task: Box&lt;dyn Fn() -&gt; Js + Send + 'static&gt;,
    callback_id: usize,
    kind: ThreadPoolTaskKind,
}

impl Task {
    fn close() -&gt; Self {
        Task {
            task: Box::new(|| Js::Undefined),
            callback_id: 0,
            kind: ThreadPoolTaskKind::Close,
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>We need a task object, which represents a task we want to finish in our thread
pool. I'll go through the types in this object in a later <a href="./8_9_infrastructure.html">chapter</a> so don't worry too much about them now if you find them
hard to grasp. Everything will be explained.</p>
<p>We also create an implementation of a <code>Close</code> task. We need this to clean up after ourselves and close down the thread pool.</p>
<p><code>|| Js::Undefined</code> might seem strange but it's only a function that returns <code>Js::Undefined</code>, we need it since we won't make <code>task</code> an <code>Option</code> just for this one case.</p>
<p>It's just so we don't have to <code>match</code> or <code>map</code> on <code>task</code> all the way through our code, it's more than enough to parse already.</p>
<h2><a class="header" href="#nodethread" id="nodethread">NodeThread</a></h2>
<p>First is <code>NodeThread</code>, which represents a thread in our thread pool. As you
see we have a <code>JoinHandle</code> (which we get when we call <code>thread::spawn</code>) and the
sending part of a channel. This channel, sends messages of the type <code>Task</code>.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>#[derive(Debug)]
struct NodeThread {
    pub(crate) handle: JoinHandle&lt;()&gt;,
    sender: Sender&lt;Task&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p>We introduced two new types here: <code>Js</code> and <code>ThreadPoolTaskKind</code>. First we'll cover <code>ThreadPoolTaskKind</code>.</p>
<p>In our example, we have three kinds of events: a <code>FileRead</code> which is a file that has been read, and an <code>Encrypt</code> that represents an operation from our <code>Crypto</code> module. The event <code>Close</code> is used to let the threads in our <code>threadpool</code> that we're closing the loop and let them finish before we exit our process.</p>
<h2><a class="header" href="#threadpooltaskkind" id="threadpooltaskkind">ThreadPoolTaskKind</a></h2>
<p>As you might understand, this object is only used in the <code>threadpool</code>.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub enum ThreadPoolTaskKind {
    FileRead,
    Encrypt,
    Close,
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#js" id="js">Js</a></h2>
<p>Next is our <code>Js</code> object. This represents different <code>Javascript</code> types, and it's only used
to make our code look more &quot;javascripty&quot;, but it's also convenient for us to
abstract over the return types of closures.</p>
<p>We'll also implement two convenience methods on this object to make our &quot;javascripty&quot;
code look a bit cleaner.</p>
<p>We know the return types already based on our modules
documentation - just like you would know it from the documentation when using a
Node module but we need to actually handle the types in Rust so this will make that just slightly easier for us.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>#[derive(Debug)]
pub enum Js {
    Undefined,
    String(String),
    Int(usize),
}

impl Js {
    /// Convenience method since we know the types
    fn into_string(self) -&gt; Option&lt;String&gt; {
        match self {
            Js::String(s) =&gt; Some(s),
            _ =&gt; None,
        }
    }

    /// Convenience method since we know the types
    fn into_int(self) -&gt; Option&lt;usize&gt; {
        match self {
            Js::Int(n) =&gt; Some(n),
            _ =&gt; None,
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#pollevent" id="pollevent">PollEvent</a></h2>
<p>Next we have the <code>PollEvent</code>. While we defined an <code>enum</code> to represent what
kind of events we could send <strong>to</strong> the <code>eventpool</code>, we define some events
that we can accept back from both our <code>epoll based</code> event queue and our <code>threadpool</code>.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Describes the three main events our epoll-eventloop handles
enum PollEvent {
    /// An event from the `threadpool` with a tuple containing the `thread id`,
    /// the `callback_id` and the data which the we expect to process in our
    /// callback
    Threadpool((usize, usize, Js)),
    /// An event from the epoll-based eventloop holding the `event_id` for the
    /// event
    Epoll(usize),
    Timeout,
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#const-runtime" id="const-runtime">const RUNTIME</a></h2>
<p>Lastly we have another convenience for us, and it's also necessary to make our
Javascript code look a bit like Javascript.</p>
<p>First we have a static variable which represents our <code>Runtime</code>. It's actually
a pointer to our <code>runtime</code> which we initialize to a null-pointer from the start.</p>
<p>We need to use unsafe to edit this. I'll explain later how this is safe, but I
also want to mention here that it could be avoided by using <a href="https://github.com/rust-lang-nursery/lazy-static.rs">lazy_static</a>
but that would both require us to add <code>lazy_static</code> as an dependency (which would
be fine since it contains no &quot;magic&quot; that we want to explain in this book) but
it also does make our code less readable, and it's complex enough.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>static mut RUNTIME: *mut Runtime = std::ptr::null_mut();
<span class="boring">}
</span></code></pre></pre>
<p>Let's now move on and look at what the heart of the runtime looks like: the main loop</p>
<h2><a class="header" href="#moving-on" id="moving-on">Moving on</a></h2>
<p>Now we've already gotten really far by introducing most of our runtime already in the first chapter. The next chapter will focus on implementing all the functionality we need for this to work.</p>
<h1><a class="header" href="#the-main-loop" id="the-main-loop">The main loop</a></h1>
<p>Let's put our event loop logic in the <code>run</code> function of our <code>Runtime</code>. The code
which we present on this chapter is the body of this <code>run</code> function.</p>
<p>The <code>run</code> function on our <code>Runtime</code> will consume <code>self</code> so it's the last thing that we'll be able to call on this instance of our <code>Runtime</code>.</p>
<p>I'll include the whole method last so you can see it all together.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>impl Runtime {
    pub fn run(mut self, f: impl Fn()) {
        ...
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#initialization" id="initialization">Initialization</a></h2>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let rt_ptr: *mut Runtime = &amp;mut self;
unsafe { RUNTIME = rt_ptr };
let mut ticks = 0; // just for us printing out

// First we run our &quot;main&quot; function
f();
<span class="boring">}
</span></code></pre></pre>
<p>The first two lines is just a <code>hack</code> we use in our code to make it &quot;look&quot; more
like javascript. We take the pointer to <code>self</code> and set it in the global
variable <code>RUNTIME</code>.</p>
<p>We could instead pass our <code>runtime</code> around but that wouldn't
be very ergonomic. Another option would be to use <code>lazy_static</code> crate to initialize this field in a slightly safer way, but we'd have to explain what <code>lazy_static</code> do to keep our promise of minimal &quot;magic&quot;.</p>
<p>To be honest, we only set this once, and it's set at the start of of our event loop and we only access this from the same thread we created it. It might not be pretty but it's safe.</p>
<p><code>ticks</code> is only a counter for us to keep track of how many times we've looped which for display.</p>
<p>The last and least visible part of this code is actually where we kick everything off, calling <code>f()</code>. <code>f</code> will be the code we wrote in the <code>javascript</code> function in the last chapter. If this is empty nothing will happen.</p>
<h2><a class="header" href="#starting-the-event-loop" id="starting-the-event-loop">Starting the event loop</a></h2>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// ===== EVENT LOOP =====
while self.pending_events &gt; 0 {
    ticks += 1;
<span class="boring">}
</span></code></pre></pre>
<p><code>self.pending_events</code> keeps track of how many pending events we have, so that when no events are left we exit the loop since our event loop is finished.</p>
<p>So where does these events come from? In our <code>javascript</code> function <code>f</code> which we introduced in the chapter <a href="./7_0_introducing_our_main_example.html">Introducing our main example</a> you probably noticed that we called functions like
<code>set_timeout</code> and <code>Fs::read</code>. These functions are defined in the Node runtime
(as they are in ours), and their main responsibility is to create tasks and register interest on events. When one of these tasks or interests are registered this counter is increased.</p>
<p><code>ticks</code> is just increasing a <code>tick</code> in the counter.</p>
<h2><a class="header" href="#1-process-timers" id="1-process-timers">1. Process timers</a></h2>
<p><code>self.process_expired_timers();</code></p>
<p>This method checks if any timers has expired. If we have timers that have expired we schedule the callbacks for the expired timers to run at the first call to <code>self.run_callbacks()</code>.</p>
<p>Worth noting here is that timers with a timeout of <code>0</code> will already have timed out by the time we reach this function so their events will be processed.</p>
<h2><a class="header" href="#2-callbacks" id="2-callbacks">2. Callbacks</a></h2>
<p><code>self.run_callbacks();</code></p>
<p>Now we could have chosen to run the callbacks in the timer <code>step</code> but since this is the next step of our loop we do it here instead.</p>
<blockquote>
<p>This step might seem unnecessary here but in Node it has a function. Some
types of callbacks will be deferred to be run on the next iteration of the loop, which means that they're
not run immediately. We won't implement
this functionality in our example but it's worth noting.</p>
</blockquote>
<h2><a class="header" href="#3-idleprepare" id="3-idleprepare">3. Idle/Prepare</a></h2>
<p>This is a step mostly used by Nodes internals. It's not important for understanding
the big picture here but I included it since it's something you see in Nodes
documentation so you know where we're at in the loop at this point.</p>
<h2><a class="header" href="#4-poll" id="4-poll">4. Poll</a></h2>
<p>This is an important step. This is where we'll receive events from our thread pool or our <code>epoll</code> event queue.</p>
<p>I refer to the <code>epoll/kqueue/IOCP</code> event queue as <code>epoll</code> here just so you know that it's not only <code>epoll</code> we're waiting for. From now on I will refer to the cross platform event queue as <code>epoll</code> in the code for brevity.</p>
<h3><a class="header" href="#calculate-time-until-next-timeout-if-any" id="calculate-time-until-next-timeout-if-any">Calculate time until next timeout (if any)</a></h3>
<p>The first thing we do is to check if we have any timers. If we have timers that will time out we calculate how many milliseconds it is to the first timer to timeout. We'll need this to make sure we don't block and forget about our timers.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let next_timeout = self.get_next_timer();

let mut epoll_timeout_lock = self.epoll_timeout.lock().unwrap();
*epoll_timeout_lock = next_timeout;
// We release the lock before we wait in `recv`
drop(epoll_timeout_lock);
<span class="boring">}
</span></code></pre></pre>
<p><code>self.epoll_timeout</code> is a <code>Mutex</code> so we need to lock it to be able to change the value it holds. Now, this is important, we need to make sure the lock is released before we <code>poll</code>. <code>poll</code> will suspend our thread, and it will try to read the value in <code>self.epoll_timeout</code>.</p>
<p>If we're still holding the lock we'll end up in a <code>deadlock</code>. <code>drop(epoll_timeout_lock)</code> releases the lock. We'll explain a bit more in detail how this works in the next chapter.</p>
<h3><a class="header" href="#wait-for-events" id="wait-for-events">Wait for events</a></h3>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>if let Ok(event) = self.event_reciever.recv() {
    match event {
        PollEvent::Timeout =&gt; (),
        PollEvent::Threadpool((thread_id, callback_id, data)) =&gt; {
            self.process_threadpool_events(thread_id, callback_id, data);
        }
        PollEvent::Epoll(event_id) =&gt; {
            self.process_epoll_events(event_id);
        }
    }
}
self.run_callbacks();
<span class="boring">}
</span></code></pre></pre>
<p>Both our <code>threadpool</code> threads and our <code>epoll</code> thread holds a <code>sending</code> part of the channel <code>self.event_reciever</code>. If either a thread in the <code>threadpool</code> finishes a task, or if the <code>epoll</code> thread receives notification that an event is ready a <code>PollEvent</code> is sent through the channel and received here.</p>
<p>This will block our main thread until something happens, <strong>or</strong> a timeout occurs.</p>
<blockquote>
<p><strong>Note:</strong>
Our <code>epoll</code> thread will read the timeout value we set in <code>self.epoll_timeout</code>, so if nothing happens
before the timeout expires it will emit a <code>PollEvent::Timeout</code> event which simply causes our main
event loop to continue and handle that timer.</p>
</blockquote>
<p>Depending on whether it was a <code>PollEvent::Timeout</code>, <code>PollEvent::Threadpool</code> or a <code>PollEvent::Epoll</code> type of event that occurred, we handle the event accordingly.</p>
<p>We'll explain these methods in the following chapters.</p>
<h2><a class="header" href="#5-check" id="5-check">5. Check</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// ===== CHECK =====
// an set immediate function could be added pretty easily but we won't that here
<span class="boring">}
</span></code></pre></pre>
<p>Node implements a check &quot;hook&quot; to the event loop next. Calls to <code>setImmediate</code>
execute here. I just include it for completeness but we won't do anything in this phase.</p>
<h2><a class="header" href="#6-close-callbacks" id="6-close-callbacks">6. Close Callbacks</a></h2>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// ===== CLOSE CALLBACKS ======
// Release resources, we won't do that here, it's just another &quot;hook&quot; for our &quot;extensions&quot;
// to use. We release resources in every callback instead.
<span class="boring">}
</span></code></pre></pre>
<p>I pretty much explain this step in the comments. Typically releasing resources,
like closing sockets, is done here.</p>
<h2><a class="header" href="#cleaning-up" id="cleaning-up">Cleaning up</a></h2>
<p>Since our <code>run</code> function basically will be the start and end of our <code>Runtime</code> we also need to clean up after ourselves. The following code makes sure all threads finish, release their resources and run all destructors:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// We clean up our resources, makes sure all destructors run.
for thread in self.thread_pool.into_iter() {
    thread.sender.send(Task::close()).expect(&quot;threadpool cleanup&quot;);
    thread.handle.join().unwrap();
}

self.epoll_registrator.close_loop().unwrap();
self.epoll_thread.join().unwrap();

print(&quot;FINISHED&quot;);
<span class="boring">}
</span></code></pre></pre>
<p>First we loop through every thread in our <code>threadpool</code> and send a &quot;close&quot; <code>Task</code> to each of them. Then We call <code>join</code> on each <code>JoinHandle</code>. Calling <code>join</code> waits for the associated thread to finish so we know
all destructors are run.</p>
<p>Next we call <code>close_loop()</code> on our <code>epoll_registrator</code> to signal the OS event queue that we want to close the loop and release our resources. We also <code>join</code> this thread so we don't end our process until all resources are released.</p>
<h2><a class="header" href="#the-final-run-function" id="the-final-run-function">The final <code>run</code> function</a></h2>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub fn run(mut self, f: impl Fn()) {
    let rt_ptr: *mut Runtime = &amp;mut self;
    unsafe { RUNTIME = rt_ptr };

    // just for us printing out during execution
    let mut ticks = 0;

    // First we run our &quot;main&quot; function
    f();

    // ===== EVENT LOOP =====
    while self.pending_events &gt; 0 {
        ticks += 1;
        // NOT PART OF LOOP, JUST FOR US TO SEE WHAT TICK IS EXECUTING
        print(format!(&quot;===== TICK {} =====&quot;, ticks));

        // ===== 2. TIMERS =====
        self.process_expired_timers();

        // ===== 2. CALLBACKS =====
        // Timer callbacks and if for some reason we have postponed callbacks
        // to run on the next tick. Not possible in our implementation though.
        self.run_callbacks();

        // ===== 3. IDLE/PREPARE =====
        // we won't use this

        // ===== 4. POLL =====
        // First we need to check if we have any outstanding events at all
        // and if not we're finished. If not we will wait forever.
        if self.pending_events == 0 {
            break;
        }

        // We want to get the time to the next timeout (if any) and we
        // set the timeout of our epoll wait to the same as the timeout
        // for the next timer. If there is none, we set it to infinite (None)
        let next_timeout = self.get_next_timer();

        let mut epoll_timeout_lock = self.epoll_timeout.lock().unwrap();
        *epoll_timeout_lock = next_timeout;
        // We release the lock before we wait in `recv`
        drop(epoll_timeout_lock);

        // We handle one and one event but multiple events could be returned
        // on the same poll. We won't cover that here though but there are
        // several ways of handling this.
        if let Ok(event) = self.event_reciever.recv() {
            match event {
                PollEvent::Timeout =&gt; (),
                PollEvent::Threadpool((thread_id, callback_id, data)) =&gt; {
                    self.process_threadpool_events(thread_id, callback_id, data);
                }
                PollEvent::Epoll(event_id) =&gt; {
                    self.process_epoll_events(event_id);
                }
            }
        }
        self.run_callbacks();

        // ===== 5. CHECK =====
        // an set immediate function could be added pretty easily but we
        // won't do that here

        // ===== 6. CLOSE CALLBACKS ======
        // Release resources, we won't do that here, but this is typically
        // where sockets etc are closed.
    }

    // We clean up our resources, makes sure all destructors run.
    for thread in self.thread_pool.into_iter() {
        thread.sender.send(Task::close()).expect(&quot;threadpool cleanup&quot;);
        thread.handle.join().unwrap();
    }

    self.epoll_registrator.close_loop().unwrap();
    self.epoll_thread.join().unwrap();

    print(&quot;FINISHED&quot;);
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#shortcuts" id="shortcuts">Shortcuts</a></h2>
<p>I'll mention some obvious shortcuts right here so you are aware of them. There are many &quot;exceptions&quot; that we don't cover in our example. We are focusing on the big picture just so we're on the same page. The <code>process.nextTick</code> function and the <code>setImmediate</code> function are two examples of this.</p>
<p>We don't cover the case where a server under heavy load might have too many callbacks to reasonably run in one <code>poll</code> which means that we could starve our I/O resources in the meantime waiting for them to finish, and probably several similar cases that a production
runtime should care about.</p>
<p>As you'll probably notice, implementing a simple version is more than enough work
for us to cover in this book, but hopefully you'll find yourself in pretty good
shape to dig further once we're finished.</p>
<h1><a class="header" href="#setting-up-our-runtime" id="setting-up-our-runtime">Setting up our runtime</a></h1>
<h2><a class="header" href="#the-threadpool" id="the-threadpool">The Threadpool</a></h2>
<p>We still don't have a threadpool or a I/O event loop running
so the next step is to set this up.</p>
<h3><a class="header" href="#lets-take-this-step-by-step" id="lets-take-this-step-by-step">Let's take this step by step</a></h3>
<p>Are you ready? Let's go!</p>
<p>The first thing we do is to add a <code>new</code> method that returns an instance of our
<code>Runtime</code>:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>impl Runtime {
    pub fn new() -&gt; Self {
<span class="boring">}
</span></code></pre></pre>
<p>First up is our thread pool. The first thing we do is
to set up a channel which our threads can use to send messages to our main thread.</p>
<p>The channel will take a tuple <code>(usize, usize, Js)</code> which will be <code>thread_id</code>,
<code>callback_id</code> and the data returned when we run our <code>Task</code>.</p>
<p>The <code>Receiver</code> part will be stored in our <code>Runtime</code>, and the <code>Sender</code> part will
be cloned to each of our threads.</p>
<p>Node defaults to 4 threads which we will copy. This is configurable in <code>Node</code>
but we will take a shortcut and hard code it:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let (event_sender, event_receiver) = channel::&lt;PollEvent&gt;();
let mut threads = Vec::with_capacity(4);

for i in 0..4 {
    let (evt_sender, evt_receiver) = channel::&lt;Task&gt;();
    let event_sender = event_sender.clone();

    let handle = thread::Builder::new()
        .name(format!(&quot;pool{}&quot;, i))
        .spawn(move || {

            while let Ok(task) = evt_receiver.recv() {
                print(format!(&quot;received a task of type: {}&quot;, task.kind));

                if let ThreadPoolTaskKind::Close = task.kind {
                    break;
                };

                let res = (task.task)();
                print(format!(&quot;finished running a task of type: {}.&quot;, task.kind));

                let event = PollEvent::Threadpool((i, task.callback_id, res));
                event_sender.send(event).expect(&quot;threadpool&quot;);
            }
        })
        .expect(&quot;Couldn't initialize thread pool.&quot;);

    let node_thread = NodeThread {
        handle,
        sender: evt_sender,
    };

    threads.push(node_thread);
}

<span class="boring">}
</span></code></pre></pre>
<p>Next up is actually creating our threads. <code>for i in 0..4</code> is an iterator over the
values 0, 1, 2 and 3. Since we push each thread to a <code>Vec</code> these values will be
treated as both the Id of the thread and the index it has in our <code>Vec</code>.</p>
<p>Next up we create a new channel which we will use to send messages <strong>to</strong> our
threads. Each thread keeps their <code>Receiver</code>, and we'll store the <code>Send</code> part
in the struct <code>NodeThread</code> which will represent a thread in our threadpool.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let (evt_sender, evt_receiver) = channel::&lt;Task&gt;();
let event_sender = event_sender.clone();
<span class="boring">}
</span></code></pre></pre>
<p>As you see here, we also clone the <code>Sender</code> part which we'll pass on to each thread
so they can send messages to our <code>main</code> thread.</p>
<p>After that's done we build our thread. We'll use <code>thread::Builder::new()</code> instead
of use <code>thread::spawn</code> since we want to give each thread a name. We'll only use this
name when we <code>print</code> from our event since it will be clear from which thread
we printed the message.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let handle = thread::Builder::new()
        .name(format!(&quot;pool{}&quot;, i))
        .spawn(move || {
<span class="boring">}
</span></code></pre></pre>
<p>You'll also see here that we <code>spawn</code> our thread finally and create a closure.</p>
<blockquote>
<p>Why do we need the <code>move</code> keyword in this closure?</p>
<p>The reason is that this closure is spawned from the main thread, so any environment
we close over needs to be owned, since it can't reference any values on the stack
of the <code>main</code> thread. I'll leave you with a relevant quote from <a href="https://doc.rust-lang.org/1.30.0/book/first-edition/closures.html#closures">chapter about
<code>closures</code> in TRPL</a></p>
<p><strong>...they give a closure its own stack frame. Without move, a closure may be
tied to the stack frame that created it, while a move closure is self-contained.</strong></p>
</blockquote>
<p>The body of our new threads are really simple, most of the lines are about printing
out information for us to see:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>while let Ok(task) = evt_receiver.recv() {
        print(format!(&quot;received a task of type: {}&quot;, task.kind));

        if let ThreadPoolTaskKind::Close = task.kind {
            break;
        };

        let res = (task.task)();
        print(format!(&quot;finished running a task of type: {}.&quot;, task.kind));

        let event = PollEvent::Threadpool((i, task.callback_id, res));
        event_sender.send(event).expect(&quot;threadpool&quot;);
    }
})
<span class="boring">}
</span></code></pre></pre>
<p>The first thing we do is to listen on our <code>Receive</code> part of the channel (remember,
we gave the <code>Send</code> part to our <code>main</code> thread). This function will actually
<code>park</code> our thread until we receive a message so it consumes no resources while
waiting.</p>
<p>When we get a <code>task</code> we first print out what kind of task we got.</p>
<p>The next thing we do is to check if this was a <code>Close</code> task, if thats true
we break out of our loop which in turn will close the thread.</p>
<p>If it wasn't a <code>Close</code> task we run our task <code>let res = (task.task)();</code>. This is where the work will
actually be done. We know from the signature of this task that it returns a <code>Js</code>
object once it's finished.</p>
<p>The next thing we do is to print out that we finished running a task, before we
send a <code>PollEvent::Threadpool</code> event with <code>thread_id</code>, the <code>callback_id</code> and the data returned as a <code>Js</code> object back
to our main thread.</p>
<p>Back in our <code>main</code> thread again we'll finally we store the <code>JoinHandle</code>, and the
<code>Send</code> part of the channel in our <code>NodeThread</code> struct and push it to our
collection of threads (which now represents our threadpool).</p>
<h2><a class="header" href="#the-epoll-thread" id="the-epoll-thread">The Epoll Thread</a></h2>
<p>This will handle our Epoll/Kqueue/IOCP thread. This thread will only wait for
incoming events reported by the OS, and once that's done it will send the Id of
the event to our main thread which in turn will actually handle the event and call
the callback.</p>
<p>The code here is a bit more involved, but we'll take it step by step below.</p>
<p>The code looks like this:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let mut poll = minimio::Poll::new().expect(&quot;Error creating epoll queue&quot;);
let registrator = poll.registrator();
let epoll_timeout = Arc::new(Mutex::new(None));
let epoll_timeout_clone = epoll_timeout.clone();

let epoll_thread = thread::Builder::new()
    .name(&quot;epoll&quot;.to_string())
    .spawn(move || {
        let mut events = minimio::Events::with_capacity(1024);

        loop {
            let epoll_timeout_handle = epoll_timeout_clone.lock().unwrap();
            let timeout = *epoll_timeout_handle;
            drop(epoll_timeout_handle);

            match poll.poll(&amp;mut events, timeout) {
                Ok(v) if v &gt; 0 =&gt; {
                    for i in 0..v {
                        let event = events.get_mut(i).expect(&quot;No events in event list.&quot;);
                        print(format!(&quot;epoll event {} is ready&quot;, event.id()));

                        let event = PollEvent::Epoll(event.id());
                        event_sender.send(event).expect(&quot;epoll event&quot;);
                    }
                }
                Ok(v) if v == 0 =&gt; {
                    print(&quot;epoll event timeout is ready&quot;);
                    event_sender.send(PollEvent::Timeout).expect(&quot;epoll timeout&quot;);
                }
                Err(ref e) if e.kind() == io::ErrorKind::Interrupted =&gt; {
                    print(&quot;received event of type: Close&quot;);
                    break;
                }
                Err(e) =&gt; panic!(&quot;{:?}&quot;, e),
                _ =&gt; unreachable!(),
            }
        }
    })
    .expect(&quot;Error creating epoll thread&quot;);
<span class="boring">}
</span></code></pre></pre>
<p>Lets start by initializing some variables:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let mut poll = minimio::Poll::new().expect(&quot;Error creating epoll queue&quot;);
let registrator = poll.registrator();
let epoll_timeout = Arc::new(Mutex::new(None));
let epoll_timeout_clone = epoll_timeout.clone();
<span class="boring">}
</span></code></pre></pre>
<p>The first thing we do is to instantiate a new <code>minimio::Poll</code>. This is the main
entry point into our <code>kqueue/epoll/iocp</code> event queue.</p>
<blockquote>
<p><code>minimio::Poll</code> does several things under the hood. It sets up a structure for
us to store some information about the state of the event queue, and most
importantly makes a syscall to the underlying OS and asks it for a handle to
either an <code>epoll</code> instance, a <code>kqueue</code> or to an <code>Completion Port</code>. We won't
register anything here yet, but we need this handle to later make sure we register
interest with the queue we're polling.</p>
</blockquote>
<p>Next up is also part of <code>minimio</code> we get a <code>Registrator</code>. This struct is &quot;detached&quot;
from the <code>Poll</code> struct, but it holds a copy of the same handle to the event queue.</p>
<p>This way we can store the <code>Registrator</code> in our main thread and send off the <code>Poll</code>
instance to our <code>epoll</code> thread. Our registrator can only register events to the
queue and that's it.</p>
<blockquote>
<p>How can <code>Registrator</code> know that the <code>epoll</code> thread hasn't stopped?</p>
<p>We'll cover this in detail in the next book, but both <code>Poll</code> and <code>Registrator</code>
holds a reference to an <code>AtomicBool</code> which only job is to indicate if the queue
is &quot;alive&quot; or not. In the <code>Drop</code> implemenation of <code>Poll</code> we set this flag to
false in which case a call to register an event will return an <code>Err</code>.</p>
</blockquote>
<p><code>epoll_timeout</code> is the time to the next timeout. If there is no more timeouts the
value is <code>None</code>. We wrap this in a <code>Arc&lt;Mutex&lt;&gt;&gt;</code>, since we'll be writing to
this from the main thread, and reading from it in the <code>epoll</code> thread.</p>
<p><code>epoll_timeout_clone</code> is just increasing the ref-count on our <code>Arc</code> so that
we can send this to our <code>epoll</code> thread.</p>
<p>Next up is spawning our thread. We do this the exact same way as for the thread
pool, but we name the thread <code>epoll</code>.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let epoll_thread = thread::Builder::new()
    .name(&quot;epoll&quot;.to_string())
    .spawn(move || {
<span class="boring">}
</span></code></pre></pre>
<p>Now we're inside the <code>epoll</code> thread and will define what this thread needs to
do to poll and handle events.</p>
<p>First we allocate a buffer to hold event objects that we get from our <code>poll</code> instance.
These objects contain information about the event that's occurred including a
<code>token</code> we pass in when we register the event. This <code>token</code> identifies what event
has occurred. In our case the token is a simple <code>usize</code>.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let mut events = minimio::Events::with_capacity(1024);
<span class="boring">}
</span></code></pre></pre>
<p>We allocate the buffer here since we only allocate this once when we do it here,
and we want to avoid allocating a new buffer on every turn of our <code>loop</code>.</p>
<p>Basically, our <code>epoll</code> thread will run a loop which consciously polls for new
events.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>loop {
<span class="boring">}
</span></code></pre></pre>
<p>The interesting logic is inside the loop, and first we read the timeout value which
should be synced with the next timeout that expires in our main loop.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let epoll_timeout_handle = epoll_timeout_clone.lock().unwrap();
let timeout = *epoll_timeout_handle;
drop(epoll_timeout_handle);
<span class="boring">}
</span></code></pre></pre>
<p>To do this we first need to <code>lock</code> the mutex so we know we have exclusive access
to the <code>timeout</code> value. Now, <code>epoll_timeout_handle</code> is of the type <code>Option&lt;i32&gt;</code>, since <code>i32</code>
implements the <code>Copy</code> trait we can dereference it, which in this case will copy,
the value and store it in our <code>timeout</code> variable.</p>
<p><code>drop(epoll_timeout_handle)</code> is not something you'll see often. The <code>MutexGuard</code> we
get in return when we call <code>epoll_timeout_clone.lock().unwrap()</code> is a <a href="https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization">RAII</a>
guard. Which means that it will hold a resource (in this case the lock on the mutex)
until it's deallocated(released). In Rust, the release happens when the value is
<code>Dropped</code> which normally is by the end of a scope (<code>{...}</code>).</p>
<p>We need to release the lock since the next call will block until an event occurs
which means our lock wouldn't have been released and we would end up in a <code>deadlock</code>
when trying to write a value to our <code>epoll_timeout</code> in our main thread.</p>
<p>The next part is a handful, but bear in mind that much of what we do here is
printing out information for us to observe.</p>
<p>Calling <code>poll</code> will block the loop until an event occurs <strong>or</strong> the timeout has
elapsed. <code>poll</code> takes in an exclusive reference to our event buffer, and an <code>Option&lt;i32&gt;</code>
as a timeout. A value of <code>None</code> will block indefinitely.</p>
<blockquote>
<p>When we say <code>block</code> here we mean that the OS parks our thread, and switches
context to another thread. However, it keeps track over that our <code>epoll</code> thread
listens to events and wakes it up again when any of the events we have registered
interests to has happened.</p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>match poll.poll(&amp;mut events, timeout) {
    Ok(v) if v &gt; 0 =&gt; {
        for i in 0..v {
            let event = events.get_mut(i).expect(&quot;No events in event list.&quot;);
            print(format!(&quot;epoll event {} is ready&quot;, event.id()));

            let event = PollEvent::Epoll(event.id());
            event_sender.send(event).expect(&quot;epoll event&quot;);
        }
    }
    Ok(v) if v == 0 =&gt; {
        print(&quot;epoll event timeout is ready&quot;);
        event_sender.send(PollEvent::Timeout).expect(&quot;epoll timeout&quot;);
    }
    Err(ref e) if e.kind() == io::ErrorKind::Interrupted =&gt; {
        print(&quot;received event of type: Close&quot;);
        break;
    }
    Err(e) =&gt; panic!(&quot;{:?}&quot;, e),
    _ =&gt; unreachable!(),
}
<span class="boring">}
</span></code></pre></pre>
<p>We <code>match</code> on the result of the <code>poll</code> so when the OS returns we choose what to do.</p>
<p>We basically have 4 cases we are concerned about:</p>
<ol>
<li>We get an Ok(n) where n is larger than 0, in this case we have events to process</li>
<li>We get an Ok(n) where n is 0, we know this either is a <code>spurious</code> wakeup or that a timeout has occurred</li>
<li>We get an Err of kind <code>Interrupted</code>, in which case we treat this as a close signal and we close the loop</li>
<li>We get an Err which is not of type <code>Interrupted</code>, we know something bad has happened, and we <code>panic!</code></li>
</ol>
<p>If you haven't seen the syntax <code>Ok(v) if v &gt; 0</code> before it's what we call a <code> match guard</code>
which lets us refine what we're matching against. In this case, we only match on
values of <code>v</code> larger than <code>0</code>.</p>
<p>For completeness I'll also explain <code>Err(ref e) if e.kind()</code>, the <code>ref</code> keyword
tells the compiler that we want a reference to <code>e</code> and don't want to take
ownership over it.</p>
<p>The last case <code>_ =&gt; unreachable!()</code> is just needed since the compiler doesn't realize that
we're matching on all values of <code>Ok()</code> here. The value is of type <code>Ok(usize)</code> so
it can't be negative, and we're telling the compiler here that we've got all
cases covered.</p>
<p>Lastly we create a <code>Runtime</code> struct and store all the data we've intialized so
far into it:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>  Runtime {
    available_threads: (0..4).collect(),
    callbacks_to_run: vec![],
    callback_queue: HashMap::new(),
    epoll_pending_events: 0,
    epoll_registrator: registrator,
    epoll_thread,
    epoll_timeout,
    event_receiver,
    identity_token: 0,
    pending_events: 0,
    thread_pool: threads,
    timers: BTreeMap::new(),
    timers_to_remove: vec![],
}
<span class="boring">}
</span></code></pre></pre>
<p>Worth noting is that we know all threads are available here so <code>(0..4).collect()</code>
will just create a <code>Vec&lt;usize&gt;</code> with the values <code>[0, 1, 2, 3]</code>.</p>
<p>In Rust, when we write...</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>...
epoll_registrator: registrator,
epoll_thread,
...
<span class="boring">}
</span></code></pre></pre>
<p>...we're in assigning <code>registrator</code> to <code>epoll_registrator</code> which is a
bit more descriptive name. Since we have a variable with the name <code>epoll_thread</code>
already we don't need to write <code>epoll_thread: epoll_thread</code> since the compiler
figures that out for us.</p>
<p>Now the final initialization code for our runtime breaks all &quot;best practices&quot; of
how long methods you should have but for our case I find it easier to write about
this if we don't need to jump between functions too much and can just cover all this
logic from a-z:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>impl Runtime {
    pub fn new() -&gt; Self {
        // ===== THE REGULAR THREADPOOL =====
        let (event_sender, event_receiver) = channel::&lt;PollEvent&gt;();
        let mut threads = Vec::with_capacity(4);

        for i in 0..4 {
            let (evt_sender, evt_receiver) = channel::&lt;Task&gt;();
            let event_sender = event_sender.clone();

            let handle = thread::Builder::new()
                .name(format!(&quot;pool{}&quot;, i))
                .spawn(move || {

                    while let Ok(task) = evt_receiver.recv() {
                        print(format!(&quot;received a task of type: {}&quot;, task.kind));

                        if let ThreadPoolTaskKind::Close = task.kind {
                            break;
                        };

                        let res = (task.task)();
                        print(format!(&quot;finished running a task of type: {}.&quot;, task.kind));

                        let event = PollEvent::Threadpool((i, task.callback_id, res));
                        event_sender.send(event).expect(&quot;threadpool&quot;);
                    }
                })
                .expect(&quot;Couldn't initialize thread pool.&quot;);

            let node_thread = NodeThread {
                handle,
                sender: evt_sender,
            };

            threads.push(node_thread);
        }

        // ===== EPOLL THREAD =====
        let mut poll = minimio::Poll::new().expect(&quot;Error creating epoll queue&quot;);
        let registrator = poll.registrator();
        let epoll_timeout = Arc::new(Mutex::new(None));
        let epoll_timeout_clone = epoll_timeout.clone();

        let epoll_thread = thread::Builder::new()
            .name(&quot;epoll&quot;.to_string())
            .spawn(move || {
                let mut events = minimio::Events::with_capacity(1024);

                loop {
                    let epoll_timeout_handle = epoll_timeout_clone.lock().unwrap();
                    let timeout = *epoll_timeout_handle;
                    drop(epoll_timeout_handle);

                    match poll.poll(&amp;mut events, timeout) {
                        Ok(v) if v &gt; 0 =&gt; {
                            for i in 0..v {
                                let event = events.get_mut(i).expect(&quot;No events in event list.&quot;);
                                print(format!(&quot;epoll event {} is ready&quot;, event.id()));

                                let event = PollEvent::Epoll(event.id());
                                event_sender.send(event).expect(&quot;epoll event&quot;);
                            }
                        }
                        Ok(v) if v == 0 =&gt; {
                            print(&quot;epoll event timeout is ready&quot;);
                            event_sender.send(PollEvent::Timeout).expect(&quot;epoll timeout&quot;);
                        }
                        Err(ref e) if e.kind() == io::ErrorKind::Interrupted =&gt; {
                            print(&quot;received event of type: Close&quot;);
                            break;
                        }
                        Err(e) =&gt; panic!(&quot;{:?}&quot;, e),
                        _ =&gt; unreachable!(),
                    }
                }
            })
            .expect(&quot;Error creating epoll thread&quot;);

        Runtime {
            available_threads: (0..4).collect(),
            callbacks_to_run: vec![],
            callback_queue: HashMap::new(),
            epoll_pending_events: 0,
            epoll_registrator: registrator,
            epoll_thread,
            epoll_timeout,
            event_receiver,
            identity_token: 0,
            pending_events: 0,
            thread_pool: threads,
            timers: BTreeMap::new(),
            timers_to_remove: vec![],
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#timers" id="timers">Timers</a></h1>
<h2><a class="header" href="#1-check-expired-timers" id="1-check-expired-timers">1. Check expired timers</a></h2>
<p>The first step in the event loop is checking for expired timers, and we do this
in the <code>self.check_expired_timers()</code> function</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>    fn process_expired_timers(&amp;mut self) {
        // Need an intermediate variable to please the borrowchecker
        let timers_to_remove = &amp;mut self.timers_to_remove;

        self.timers
            .range(..=Instant::now())
            .for_each(|(k, _)| timers_to_remove.push(*k));

        while let Some(key) = self.timers_to_remove.pop() {
            let callback_id = self.timers.remove(&amp;key).unwrap();
            self.callbacks_to_run.push((callback_id, Js::Undefined));
        }
    }

    fn get_next_timer(&amp;self) -&gt; Option&lt;i32&gt; {
        self.timers.iter().nth(0).map(|(&amp;instant, _)| {
            let mut time_to_next_timeout = instant - Instant::now();
            if time_to_next_timeout &lt; Duration::new(0, 0) {
                time_to_next_timeout = Duration::new(0, 0);
            }
            time_to_next_timeout.as_millis() as i32
        })
    }
<span class="boring">}
</span></code></pre></pre>
<p>The first thing to note here is that we check <code>self.timers</code> and to understand the
rest of the syntax we'll have to look what kind of collection this is.</p>
<p>Now I chose a <code>BTreeMap&lt;Instant, usize&gt;</code> for this collection. The reason is that
I want to have many <code>Instant</code>'s chronologically. When I add a timer, I calculate
at what instance it's supposed to be run and I add that to this collection.</p>
<blockquote>
<p>BTrees are a very good data structure when you know that your keys will be ordered.</p>
</blockquote>
<p>Choosing a <code>BTreeMap</code> here allows me to get a range <code>range(..=Instant::now())</code>
which is from the start of the map, up until or equal to the instant NOW.</p>
<p>Now I take every key in this range and add it to <code>timers_to_remove</code>, and the reason
for this is that I found no good way to both get a range and remove the key's in one
operation without allocating a small buffer every time. You can iterate over the range
but due to the ownership rules you can't remove them at the same time, and we want to
remove the timers, that we're done with.</p>
<p>The event loop will run repeatedly, so avoiding any allocations inside the loop is smart. There is no need to have this overhead.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>while let Some(key) = timers_to_remove.pop() {
    let callback_id = self.timers.remove(&amp;key).unwrap();
    self.callbacks_to_run.push((callback_id, Js::Undefined));
}
<span class="boring">}
</span></code></pre></pre>
<p>The next step is to take every timer that has expired, remove the timer from our <code>self.timers</code> collection and get their <code>callback_id</code>.</p>
<p>As I explained in the previous chapter, this is an unique Id for this callback. What's
important here is that we don't run the callback <strong>immediately</strong>. Node actually registers callbacks to be run on the next <code>tick</code>. An exception is the timers since they either have timed out or is a timer with a timeout of <code>0</code>. In this case a timer will not wait for the next tick if it has timed out (or  if it has a timeout of <code>0</code>) but instead they will be invoked immediately as you'll see next.</p>
<p>Anyway, for now we add the callback id's to <code>self.callbacks_to_run</code>.</p>
<p>Before we continue, let's recap by looking what members of the <code>Runtime</code> struct
we used here:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub struct Runtime {
    pending_events: usize,
    callbacks_to_run: Vec&lt;(usize, Js)&gt;,
    timers: BTreeMap&lt;Instant, usize&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#callbacks" id="callbacks">Callbacks</a></h1>
<h2><a class="header" href="#2-process-callbacks" id="2-process-callbacks">2. Process callbacks</a></h2>
<p>The next step is to handle any callbacks we've scheduled to run.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>fn run_callbacks(&amp;mut self) {
    while let Some((callback_id, data)) = self.callbacks_to_run.pop() {
        let cb = self.callback_queue.remove(&amp;callback_id).unwrap();
        cb(data);
        self.pending_events -= 1;
    }
}
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p>Not all of Nodes callbacks are processed here. Some callbacks is called
directly in the <code>poll</code> phase we'll introduce below. It's not difficult to implement
but it adds unnecessary complexity to our example so we schedule all callbacks to be
run in this step of the process. As long as you know this is an oversimplification
you're going to be alright :)</p>
</blockquote>
<p>Here we <code>pop</code> off all callbacks that are scheduled to run. As you see from our last update on the <code>Runtime</code> struct. <code>callbacks_to_run</code> is an array of callback_id and an argument type of <code>Js</code>.</p>
<p>So when we've got a <code>callback_id</code> we find the corresponding callback we have stored in <code>self.callback_queue</code> and remove the entry. What we get in return is a callback of type
<code>Box&lt;dyn FnOnce(Js)&gt;</code>. We're going to explain this type more later but it's basically a closure stored on the heap that takes one argument of type <code>Js</code>.</p>
<p><code>cb(data)</code> runs the code in this closure. After it's done it's time to decrease our counter of pending events: <code>self.pending_events -= 1;</code>.</p>
<blockquote>
<p>Now, this step is important. As you might understand, any long running code in this callback is going to block our <code>event loop</code>, preventing it from progressing. So no new callbacks are handled and no new events are registered. This is why it's bad to write code that blocks the event loop.</p>
</blockquote>
<p>Let's recap by looking at what members of the <code>Runtime</code> struct we used here:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub struct Runtime {
    callback_queue: HashMap&lt;usize, Box&lt;dyn FnOnce(Js)&gt;&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#the-threadpool-1" id="the-threadpool-1">The Threadpool</a></h1>
<p>The threadpool is where we'll process the CPU intensive tasks and the I/O tasks
which can't be reasonably handled by <code>epoll, kqueue or IOCP</code>. One of these tasks
are file system operations.</p>
<p>The reason for doing file I/O in the thread pool is complex, but the main takeaway
is that due to how files are cached and how the hard drive works, most often the
file I/O will be <code>Ready</code> almost immediately, so waiting for that in a event queue
has very little effect in practice.</p>
<p>The second reason is that while Windows do have a completion based model, Linux
and macOS doesn't. Reading a file into a buffer which your process controls can take some
time, and if we do that in our main loop it will block a little bit which we really try
to avoid.</p>
<p>By doing this in a thread pool we make sure that these operations won't block our
main event loop and only notify us once the data is ready for us in memory.</p>
<p>The code we need to add to process events from the thread pool is short an simple:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>fn process_threadpool_events(&amp;mut self, thread_id: usize, callback_id: usize, data: Js {
    self.callbacks_to_run.push((callback_id, data));
    self.available_threads.push(thread_id);
}
<span class="boring">}
</span></code></pre></pre>
<p>We take <code>thread_id</code>, <code>callback_id</code> and <code>data</code> as arguments. We get this through
the channel we have shared with our <code>threadpool</code> threads.</p>
<p>Once we have that information we push our callbacks into the queue of <code>callbacks_to_run</code>
which will run on the next call to <code>run_callbacks</code> we went through in the previous chapter.</p>
<p>Last we take the <code>id</code> of the thread that sent the finished task and put it into
our pool of available threads.</p>
<h1><a class="header" href="#io-event-queue" id="io-event-queue">I/O event queue</a></h1>
<p>The I/O event queue is what handles most of our I/O tasks. Now we'll go through
how we register events to that queue later on, but once an event is ready we
it sends the <code>event_id</code> through our channel.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>fn process_epoll_events(&amp;mut self, event_id: usize) {
    self.callbacks_to_run.push((event_id, Js::Undefined));
    self.epoll_pending_events -= 1;
}
<span class="boring">}
</span></code></pre></pre>
<p>As we'll see later on, the way we designed this, we actually made our <code>event_id</code>
and <code>callback_id</code> the same value since both represents an unique value for this
event. It simplifies things slightly for us.</p>
<p>We add the <code>callback_id</code> to the collection of callbacks to run. We pass
in <code>Js::Undefined</code> since we'll not actually pass any data along here. You'll see
why when we reach the `<a href="./8_3_http_module.html">Http module</a> chapter, but the main
point is that the I/O queue doesn't return any data itself, it just tells us that
data is ready to be read.</p>
<p>Lastly it's only for our own bookkeeping we decrement the count of outstanding
<code>epoll_pending_events</code> so we keep track of how many events we have pending.</p>
<blockquote>
<p><strong>Why even keep track of how many <code>epoll_events</code> are pending?</strong>
We don't use this value here, but I added it to make it easier to create
some <code>print</code> statements showing the status of our runtime at different points.
However, there are good reasons to keep track of these events even if we don't use them.</p>
<p>One area we're taking shortcuts on all the way here is security. If someone were
to build a public facing server out of this, we need to account for slow networks
and malicious users.</p>
<p>Since we use <code>IOCP</code>, which is a completion based model, we allocate memory for
a buffer for each <code>Read</code> or <code>Write</code> event. When we lend this memory to the OS,
we're in a weird situation. We own it, but we can't touch it. There are several
ways in which we could register interest in more events than occur, and thereby
allocating memory that is just held in the buffers. Now if someone wanted to crash
our server, they could cause this intentionally.</p>
<p>A good practice is therefore to create a &quot;high watermark&quot; by keeping track of
the number of pending events, and when we reach that watermark, we queue events
instead of registering them with the OS.</p>
<p>By extension, this is also why you should <strong>always</strong> have a timeout on these events
so that you at some point can reclaim the memory and return an timeout error if
necessary.</p>
</blockquote>
<h1><a class="header" href="#cleaning-up-1" id="cleaning-up-1">Cleaning up</a></h1>
<p>Lastly we clean up after ourselves by joining all our threads so we know that
all destructors have ran without errors after we're finished.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// We clean up our resources, make sure all destructors run.
for thread in self.thread_pool.into_iter() {
    thread.sender.send(Event::close()).expect(&quot;threadpool cleanup&quot;);
    thread.handle.join().unwrap();
}

self.epoll_registrator.close_loop().unwrap();
self.epoll_thread.join().unwrap();

print(&quot;FINISHED&quot;);
<span class="boring">}
</span></code></pre></pre>
<p>Before we join the thread on our thread pool we send a <code>Close</code> event which
unparks the thread and exits the loop we're in.</p>
<p>How we clean up after ourselves in <code>minimio</code> will be covered in the book that
covers this topic specifically.</p>
<p>If you want to read more about why it's a good practice to join all threads
and clean up after us before we exit, I can recommend the article <a href="https://matklad.github.io/2019/08/23/join-your-threads.html">Join Your Threads</a>
written by <a href="https://matklad.github.io/">@matklad</a>.</p>
<h1><a class="header" href="#infrastructure" id="infrastructure">Infrastructure</a></h1>
<p>Now, for everything to work we need some helpers to make our infrastructure work.</p>
<p>First of all, we need a way to get the <code>id</code> of an available thread.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>   fn get_available_thread(&amp;mut self) -&gt; usize {
        match self.available_threads.pop() {
            Some(thread_id) =&gt; thread_id,
            // We would normally return None and not panic!
            None =&gt; panic!(&quot;Out of threads.&quot;),
        }
    }
<span class="boring">}
</span></code></pre></pre>
<p>As you see, we take one huge shortcut here. If we run out of threads, we <code>panic!</code>.
This is not good, and we should rather implement logic to queue these requests
and run them as soon as a thread is available. However, our code is already getting
long, and it's not very important for our goal of learning about <code>async</code>.</p>
<p>Maybe this implementing such a queue is a good reader-exercise? Feel free to fork
the repository and go ahead :)</p>
<p>The next thing we need to do is to create an unique identity for our callbacks.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// If we hit max we just wrap around
fn generate_identity(&amp;mut self) -&gt; usize {
    self.identity_token = self.identity_token.wrapping_add(1);
    self.identity_token
}

fn generate_cb_identity(&amp;mut self) -&gt; usize {
    let ident = self.generate_identity();
    let taken = self.callback_queue.contains_key(&amp;ident);

    // if there is a collision or the identity is already there, we loop until we
    // find a new one. We don't cover the case where there are `usize::max_value()`
    // callbacks waiting, since if we're fast and queue a new event
    // every nanosecond, that would still take 585 years on a 64 bit system.
    if !taken {
        ident
    } else {
        loop {
            let possible_ident = self.generate_identity();
            if self.callback_queue.contains_key(&amp;possible_ident) {
                break possible_ident;
            }
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>The function <code>generate_cb_identity</code> is where it all happens, <code>genereate_identity</code> is just
a small function so we try to avoid the long functions we had in the introduction.</p>
<blockquote>
<p>Now, there are some important considerations to be aware of. Even though we use
several threads, we use a regular <code>usize</code> here and the reason for that is that
it's only one thread that will be generating Id's. This could cause problems if
several threads tried to <code>read</code> and <code>generate</code> new Id's at the same time.</p>
</blockquote>
<p>We use the <code>wrapping_add</code> method on <code>usize</code> to get the next Id, this means that
when we reach <code>18446744073709551615</code> we wrap around to 0 again.</p>
<p>We do check of our callback_queue contains our key (even though that is unlikely
by design), and if it's taken we just generate a new one until we find a available
one.</p>
<p>Next up is the method we use to add a callback to our <code>callback_queue</code>:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// Adds a callback to the queue and returns the key
fn add_callback(&amp;mut self, ident: usize, cb: impl FnOnce(Js) + 'static) {
    let boxed_cb = Box::new(cb);
    self.callback_queue.insert(ident, boxed_cb);
}
<span class="boring">}
</span></code></pre></pre>
<p>If you haven't seen the signature <code>cb: impl FnOnce(Js) + 'static</code> before I'll
explain it briefly here.</p>
<p>The <code>impl ...</code> means that we accept an arguments that implements the trait <code>FnOnce(Js)</code>
with a <code>'static</code> lifetime. <a href="https://doc.rust-lang.org/std/ops/trait.FnOnce.html">FnOnce</a> is
a trait implemented by <code>closures</code>. There are three main traits a <code>closure</code> can implement
in Rust and <code>FnOnce</code> is the one you'll use if you plan on consume an instance from
the environment.</p>
<p>Since you consume the variable a <code>closure</code> implementing <code>FnOnce</code> can only be called
once. Our closure will take ownership over resources we create in our <code>main</code> thread
and consume it. We want this since once consumed, the resources we used will be cleaned
up as a result of Rusts <code>RAII</code> pattern. It's implicit that <code>FnOnce</code> returns <code>()</code> in this
case so we don't have to write <code>FnOnce(Js) -&gt; ()</code>.</p>
<p>Since callbacks are meant to only be called once, this is a perfectly fine bound
for us to use here.</p>
<p>Now, traits doesn't have a size so for the compiler to be able to allocate space
for it on the stack we either need to take a reference <code>&amp;FnOnce(Js)</code> or place it
on the heap using <code>Box</code>. We do the latter since that's the only thing that makes
sense for our use case. Box is a pointer to a heap allocated variable which we do
know the size of so we store that reference in our <code>callback_queue</code> HashMap.</p>
<blockquote>
<p>What makes a closure?
A function in rust can be defined as easily as <code>|| { }</code>. If this is all we write
it's the same as a function pointer, equivalent to just referencing <code>my_method</code> (without parenthesis).
It becomes a <code>closure</code> as soon as you &quot;close&quot; over your environment by referencing
variables that's not owned by the <code>function</code>.</p>
<p><code>Fn</code> traits are automatically implemented, and whether it implements <code>Fn</code>, <code>FnMut</code>
or <code>FnOnce</code> depend whether you take ownership over a non-copy variable, take a shared
reference <code>&amp;</code> or an exclusive reference <code>&amp;mut</code> (often called a mutable reference).</p>
</blockquote>
<p>Now that we got some closure basics out of the way we can move on. The next method
is how we register <code>I/O</code> work. This is how we register an <code>epoll</code> event with our runtime:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub fn register_event_epoll(&amp;mut self, token: usize, cb: impl FnOnce(Js) + 'static) {
    self.add_callback(token, cb);

    print(format!(&quot;Event with id: {} registered.&quot;, token));
    self.pending_events += 1;
    self.epoll_pending_events += 1;
}
<span class="boring">}
</span></code></pre></pre>
<p>The first thing we do is to add the callback to our <code>callback_queue</code>, calling the
method we explained previously. Next we do a print statement, just since we want
to print out the flow of our program we need to add this at strategic places.</p>
<blockquote>
<p>One important thing to note here. Our <code>token</code> in this case is already guaranteed
to be unique. We generate it in the <code>Http</code> module (which is the only one registering
events by using this method in our example). The reason for this will become clear
in a few short chapters. Just note that we don't need to call <code>generate_cb_identity</code> here.</p>
</blockquote>
<p>We increase the counters on both <code>pending_events</code> and <code>epoll_pending_events</code>.</p>
<p>Our next method registers work for the thread pool</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub fn register_event_threadpool(
    &amp;mut self,
    task: impl Fn() -&gt; Js + Send + 'static,
    kind: ThreadPoolTaskKind,
    cb: impl FnOnce(Js) + 'static,
) {
    let callback_id = self.generate_cb_identity();
    self.add_callback(callback_id, cb);

    let event = Task {
        task: Box::new(task),
        callback_id,
        kind,
    };

    // we are not going to implement a real scheduler here, just a LIFO queue
    let available = self.get_available_thread();
    self.thread_pool[available].sender.send(event).expect(&quot;register work&quot;);
    self.pending_events += 1;
}
<span class="boring">}
</span></code></pre></pre>
<p>Let's first have a look at the arguments to this function (aside from <code>&amp;mut self</code>).</p>
<p><code>task: impl Fn() -&gt; Js + Send + 'static</code> is a task we want to run on a separate
thread. This closure has the bond: <code>Fn() -&gt; Js + Send + 'static</code> which means
it's a <code>closure</code> that takes no arguments, but returns a type of <code>Js</code>. It needs to
be <code>Send</code> since we're sending this task to another thread.</p>
<p><code>kind: ThreadPoolTaskKind</code> lets us know what kind of task this. We do this for
two reasons:</p>
<ol>
<li>We need to be able to signal a <code>Close</code> event to our threads</li>
<li>We want to be able to print the kind of task each event received.</li>
</ol>
<p>As you understand, we don't have to create a <code>Kind</code> for every task, but since we
want to print out what the thread received we need some way of judging what kind
of task each thread received.</p>
<p>The last argument <code>cb: impl FnOnce(Js) + 'static</code> is our callback. It's not a coincidence
that our <code>task</code> returns a type of <code>Js</code> and our callback takes a <code>Js</code> as an argument. The
result of the work we do in our thread is the input to our callback. This closure doesn't
need to be <code>Send</code> since we don't pass the callback itself to the thread pool.</p>
<p>Next we generate a new identity with <code>self.generate_cb_identity()</code> and we add the
callback to our callback queue.</p>
<p>Then we construct a new <code>Event</code>, and as I have shown earlier, we need to <code>Box</code> the
closure.</p>
<p>Now, the last part could be made arbitrarily complex. This is where you decide how
you want to schedule your work to the thread pool. In our case we just get an
available thread (and <code>panic!</code> if we're out of thread - ouch), and we send our task
to the thread which then runs it until it's finished.</p>
<p>You could make priorities based on <code>TaskKind</code>, you could try to decide which tasks
are short and which are long and prioritize them based on load. A lot of exciting
things could be done here. We will choose the simplest possible one though, and just
push them directly to a thread in the order they come.</p>
<p>The last part of the &quot;infrastructure&quot; is a function to set a timeout.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>    fn set_timeout(&amp;mut self, ms: u64, cb: impl Fn(Js) + 'static) {
        // Is it theoretically possible to get two equal instants? If so we'll have a bug...
        let now = Instant::now();
        let cb_id = self.generate_cb_identity();
        self.add_callback(cb_id, cb);
        let timeout = now + Duration::from_millis(ms);
        self.timers.insert(timeout, cb_id);
        self.pending_events += 1;
        print(format!(&quot;Registered timer event id: {}&quot;, cb_id));
    }
<span class="boring">}
</span></code></pre></pre>
<p>Set timeout uses <code>std::time::Instant</code> to get a representation of &quot;now&quot;. It's the first
thing we do since the user expects the timeout to be calculated from &quot;now&quot;, and some
of our operations here might take a little time.</p>
<p>We generate an identity for the callback <code>cb</code> passed in to <code>set_timeout</code> and add
that callback to our callback queue.</p>
<p>We add the <code>duration</code> in milliseconds to our <code>Instant</code> so we know at what time
our timeout times out.</p>
<p>We insert the <code>callback_id</code> instant to our <code>BtreeMap</code> with the calculated <code>Instant</code> as
the key.</p>
<p>We increase the counter for <code>pending_events</code> and print out a message for us to
be able to follow the flow of our program.</p>
<blockquote>
<p>This might be a good time to talk briefly about our choice of a <code>BTreeMap</code> as
the collection we store timers in.</p>
<p>From the documentation we can read <em>&quot;In theory, a binary search tree (BST) is the optimal choice for a sorted map, as a perfectly balanced BST performs the theoretical minimum amount of comparisons necessary to find an element (log2n).&quot;</em>
Now, this isn't a Binary Tree but a BTree. While a BST allocates one node for each value, a BTree allocates
a small <code>Vec</code> of values for each node. Modern computers reads much more data than we normally ask for
into its caches, and thats one reason they love contiguous parts of memory. A BTree will result in
a more optimal &quot;cache efficiency&quot; which often trumps the gains of the theoretically more optimal
algorithm in a true BST.</p>
<p>Lastly, since we're talking about searching sorted collections here, and timeouts, is a perfect example
of such, we'll of course use this when it's so readily available to us in Rusts standard library.</p>
</blockquote>
<h1><a class="header" href="#modules" id="modules">Modules</a></h1>
<p>We're soon at the end now. Actually, our core runtime is finished, but we need
a way to actually make it work with our &quot;javascripty&quot; code.</p>
<p>The modules here would be the equivalent of &quot;C++&quot; modules in Node, and in theory
we could allow for people to make modules that register work to be either done in
our <code>threadpool</code> or our <code>epoll</code> event queue.</p>
<p>The first one we'll cover is also the simplest, but it will also introduce us to
why we stored a pointer to our <code>Runtime</code> as a global constant.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub fn set_timeout(ms: u64, cb: impl Fn(Js) + 'static) {
    let rt = unsafe { &amp;mut *(RUNTIME as *mut Runtime) };
    rt.set_timeout(ms, cb);
}
<span class="boring">}
</span></code></pre></pre>
<p>To be able in our &quot;javascript&quot; to just call <code>set_timeout(...)</code> without injecting our
runtime into our <code>javascript</code> function (which we could do if we wanted to write
better Rust) we need to dereference a mutable pointer to our runtime.</p>
<p>Now, we know this is safe for two reasons.</p>
<ol>
<li><code>RUNTIME</code> is always access from one thread</li>
<li>We know that all calls to our <code>modules</code> will be in <code>Runtime.run(...)</code> at which point we know that <code>RUNTIME</code> will be a valid pointer</li>
</ol>
<p>Now it's not pretty, and that's also why I explain why we do it, and why it's safe here. Normally, all <code>unsafe</code>
code should contain a reason why it's used, and why it's safe in a comment.</p>
<p>Once we have referenced the pointer to <code>RUNTIME</code> we can call methods on it and in
this case we simply call <code>rt.set_timeout(ms, cb)</code> which sets a timeout as you saw
in the last chapter.</p>
<h1><a class="header" href="#file-module" id="file-module">File module</a></h1>
<p>The <code>Fs</code> module contains File operations. Right now we only expose a <code>read</code> method
since that's all we need. As you can imagine we can add all sorts of methods here.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct Fs;
impl Fs {
    fn read(path: &amp;'static str, cb: impl Fn(Js) + 'static) {
        let work = move || {
            // Let's simulate that there is a very large file we're reading allowing us to actually
            // observe how the code is executed
            thread::sleep(std::time::Duration::from_secs(1));
            let mut buffer = String::new();
            fs::File::open(&amp;path)
                .unwrap()
                .read_to_string(&amp;mut buffer)
                .unwrap();
            Js::String(buffer)
        };
        let rt = unsafe { &amp;mut *RUNTIME };
        rt.register_event_threadpool(work, ThreadPoolTaskKind::FileRead, cb);
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>We simply create an empty <code>struct Fs;</code>. This is one of <a href="https://doc.rust-lang.org/nomicon/exotic-sizes.html#zero-sized-types-zsts">Rusts Zero Sized Types (ZST)</a> and does not occupy any space, but it's still useful for us.</p>
<p>The <code>read</code> method takes a file path and a callback as arguments. The callback will get
called once the operation is finished and accepts a <code>Js</code> object as an input.</p>
<p><code>let work = move || {...</code> is a closure. This closure is the actual <code>Task</code> that we
want to run on our <code>threadpool</code>. None of the code in <code>work</code> will actually run here,
we only define what we want to do when <code>work()</code> gets called.</p>
<p>In our closure we first wait for a second. These operations are so fast (and our file is so small)
that if we want to observe what's going on in any meaningful way we need to slow things down. Let's pretend
its a huge file that takes a second to read.</p>
<p>We read the file into a buffer and then return <code>Js::String(buffer)</code>.</p>
<blockquote>
<p>You might remember from the <a href="./8_9_infrastructure.html">Infrastructure chapter</a> that
our <code>register_work</code> method received a task argument <code>task: impl Fn() -&gt; Js + Send + 'static</code>.
As you see here, our closure returns a <code>Js</code>object and takes no arguments, which means
it conforms to this signature. The <code>Fn</code> trait will be automatically derived.
<code>Send</code> is also an automatically derived trait, which means that we can't implement
<code>Send</code>. However if we tried to send types that are <code>!Send</code> to our thread by
referencing them in our closure we would get an error.</p>
</blockquote>
<p>The last part is that we dereference our runtime and call <code>rt.register_work(work, ThreadPoolTaskKind::FileRead, cb)</code>
to register the task with our <code>threadpool</code>.</p>
<h2><a class="header" href="#bonus-material" id="bonus-material">Bonus material</a></h2>
<p>You might be wondering why we (and <code>libuv</code> and Rusts own <code>tokio</code>) do file operations
in the <code>threadpool</code> and not in our <code>epoll-event-queue</code>? It's I/O  isn't it?</p>
<p>There are actually several reasons for this:</p>
<p>First and foremost. The OS will cache a lot of files that are frequently accessed,
and when the data is cached it will be available immediately. There is no real I/O in
such a case. In addition, it seems that most programs tend to access the same files over and over
so a cache hit will often be the case. Think of a web server for example, there's often
a very limited amount of data accessed on disk.</p>
<p>Now if we say that data is cached most of the time, so it's readily available, it can
be more expensive to actually register an event with the <code>epoll-event-queue</code> - get an immediate
notification that the event is <code>Ready</code> and then perform the read. It might actually be faster to just
read it in a blocking manner right away.</p>
<p>However, in our design the file will be read on our main thread, which means that
if it's a large file it will still take some time to read it from the OS cache to
your process memory (your buffer) and that will block our entire event loop.</p>
<p>Better do that in the thread pool.</p>
<p>Secondly, the support for async file operations is limited and to a varying degree
well implemented. The only system that does this pretty good is Windows since it
uses a <code>completion</code> based model (which means it can let you know when the data is
read into your buffer).</p>
<blockquote>
<p>With the introduction of <a href="https://kernel.dk/io_uring.pdf">io_uring</a> Linux has arguably made
significant improvements in this regard, and now supports a <em>completion</em> based model as well. At the
time of writing this book it's still early but the reports so far has been very promising. We
might expect to see changes in the way we handle cross platform event loops in the future due to the
fact that there is now two major systems supporting high performance completion based models.</p>
</blockquote>
<p>It makes sense for a completion based model to try to do this asynchronously, but
since the real effect are so small and the code complexity is high (especially when you're
writing a server that is cross platform) most implementations find that using a thread pool
gives good enough performance.</p>
<p>To sum it all up:</p>
<p><strong>Threadpool:</strong></p>
<ul>
<li>Less code complexity</li>
<li>Good performance</li>
<li>Very little penalty in most use cases (like web servers)</li>
<li>Synchronous file operations are well optimized on most platforms</li>
</ul>
<p><strong>Async file I/O:</strong></p>
<ul>
<li>Increased code complexity</li>
<li>Poor and limited APIs (Linux and macOS has different limitations)</li>
<li>Weak platform support (does not work very well with a readiness based model)</li>
<li>Little to no real gain for most use cases</li>
</ul>
<p>You want to know more you say? Of course, <a href="https://blog.libtorrent.org/2012/10/asynchronous-disk-io/">I have an article for you</a> if you want to get to know even more about this specific topic.</p>
<h1><a class="header" href="#crypto-module" id="crypto-module">Crypto module</a></h1>
<p>As you'll soon understand, we won't actually cover cryptography here, but we'll
simulate a CPU intensive operation that would block if not run on the threadpool.</p>
<p>Without further ado: The Glorious Crypto Module</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct Crypto;
impl Crypto {
    fn encrypt(n: usize, cb: impl Fn(Js) + 'static + Clone) {
        let work = move || {
            fn fibonacchi(n: usize) -&gt; usize {
                match n {
                    0 =&gt; 0,
                    1 =&gt; 1,
                    _ =&gt; fibonacchi(n - 1) + fibonacchi(n - 2),
                }
            }

            let fib = fibonacchi(n);
            Js::Int(fib)
        };

        let rt = unsafe { &amp;mut *RUNTIME };
        rt.register_event_threadpool(work, ThreadPoolTaskKind::Encrypt, cb);
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>Well, um, this is disappointing if you're into crypto. I'm sorry. The logic here is not very interesting, we take a number <code>n</code> as argument and calculate
the n'th fibonacchi number recursively. A famous and inefficient way of putting your
CPU to work.</p>
<p>The overall function will be familiar and look very similar to what we do in our <code>Fs</code> module since this
module also sends work to the <code>threadpool</code>.</p>
<p>We create a closure (remember that none of the code in the closure will be
run here, but it will get called in one of our worker threads).</p>
<p>Once that is finished we return the result as <code>Js::Int(fib)</code>.</p>
<p>Lastly we dereference our <code>RUNTIME</code> and we call <code>rt.register_work(work, ThreadPoolTaskKind::Encrypt, cb)</code> to register our task with the <code>threadpool</code>.</p>
<p>Let's move on, we're very soon at the finish line.</p>
<h1><a class="header" href="#http-module" id="http-module">Http module</a></h1>
<p>The <code>Http</code> module is probably the one I personally find interesting. There is
two reasons for this:</p>
<ol>
<li>We create a Http GET request from scratch and wait for the response</li>
<li>The method <code>epoll_registrator.register()</code> is the result of a huge amount of research and work to get working. Look forward to the next book where we dive into that.</li>
</ol>
<p>Let's look at the code first and then step through it:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>struct Http;
impl Http {
    pub fn http_get_slow(url: &amp;str, delay_ms: u32, cb: impl Fn(Js) + 'static + Clone) {
        let rt: &amp;mut Runtime = unsafe { &amp;mut *RUNTIME };
        // Don't worry, http://slowwly.robertomurray.co.uk is a site for simulating a delayed
        // response from a server. Perfect for our use case.
        let adr = &quot;slowwly.robertomurray.co.uk:80&quot;;
        let mut stream = minimio::TcpStream::connect(adr).unwrap();

        let request = format!(
            &quot;GET /delay/{}/url/http://{} HTTP/1.1\r\n\
             Host: slowwly.robertomurray.co.uk\r\n\
             Connection: close\r\n\
             \r\n&quot;,
            delay_ms, url
        );

        stream
            .write_all(request.as_bytes())
            .expect(&quot;Error writing to stream&quot;);

        let token = rt.generate_cb_identity();
        rt.epoll_registrator
            .register(&amp;mut stream, token, minimio::Interests::READABLE)
            .unwrap();

        let wrapped = move |_n| {
            let mut stream = stream;
            let mut buffer = String::new();
            stream
                .read_to_string(&amp;mut buffer)
                .expect(&quot;Stream read error&quot;);
            cb(Js::String(buffer));
        };

        rt.register_event_epoll(token, wrapped);
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>First we call the method <code>http_get_slow</code> since we're simulating a slow response, again
this is for us to see and control how the events will occur since we're trying to learn.</p>
<p>In the function body, the first thing we do is dereference our runtime. We need to use a bit of its functionality here so we do this right away.</p>
<p>The next step is to create a <code>minimio::TcpStream</code>.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let adr = &quot;slowwly.robertomurray.co.uk:80&quot;;
let mut stream = minimio::TcpStream::connect(adr).unwrap();
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p><strong>Now why not the regular <code>TcpStream</code> from the standard library?</strong></p>
<p>Well, you do remember that <code>kqueue</code> and <code>epoll</code> are readiness based and <code>IOCP</code> is
completion based right?</p>
<p>Well, to have single ergonomic API for all platforms
we need to abstract over something and we (like <code>mio</code>) choose to abstract over <code>TcpStream</code>.</p>
<p>While <code>kqueue</code> and <code>epoll</code> can just read when a <code>Read</code> event is ready, we need the <code>TcpStream</code> to create a buffer that it hands over to the OS on Windows. So when we call <code>TcpStream</code> read, we read from this buffer on Windows.</p>
</blockquote>
<p>We connect to <code>slowwly.robertomurray.co.uk:80</code> which is just a site <a href="https://github.com/rob-murray">Robert Murray</a> has created
to simulate slow responses. He's kind enough to let us all use it. We can choose the delay we want on
each request.</p>
<p>Next we construct a http <code>GET</code> request. The line breaks, and spacing is important here
as is the two blank lines at the bottom.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let request = format!(
            &quot;GET /delay/{}/url/http://{} HTTP/1.1\r\n\
             Host: slowwly.robertomurray.co.uk\r\n\
             Connection: close\r\n\
             \r\n&quot;,
            delay_ms, url
        );
<span class="boring">}
</span></code></pre></pre>
<p>You might wonder why we use <code>\r\n\</code> as line breaks here instead of the standard <code>\n</code>?</p>
<p>This is because a http <code>GET</code> request expects <code>CRLF</code> and not just <code>LF</code>, using only <code>\n</code> will
not result in a valid <code>GET</code> request.</p>
<p>We construct the request by passing in the delay we want and the address we
want to be redirected to.</p>
<p>Next we write this request to our <code>TcpStream</code>. In a real implementation this would
have been done by issuing the write in an async manner and then register an event
when the write has happened.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>stream
    .write_all(request.as_bytes())
    .expect(&quot;Error writing to stream&quot;);
<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p>We write it blocking here since by implementing both <code>read</code> and <code>write</code> we'll have
to create much more code, and the <strong>understanding</strong> of how this works will not really
benefit that much (the understanding of how a web server works would though but thats
beyond our scope today)</p>
</blockquote>
<p>Now we get to the exciting part.</p>
<p>First we need to generate a <code>token</code>. This token will follow our <code>event</code> and be passed
on to the OS. When the OS returns it also returns this token so we know what event
occurred.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let token = rt.generate_cb_identity();
<span class="boring">}
</span></code></pre></pre>
<p>For convenience we use the same token as our <code>callback_id</code> since it will be unique, and
events &lt;-&gt; callbacks will map 1:1 the way we have designed this.</p>
<p>Our next call actually issues a syscall to the underlying OS and registers our
interest in an event of type <code>Readable</code>.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>rt.epoll_registrator
            .register(&amp;mut stream, token, minimio::Interests::readable())
            .unwrap();
<span class="boring">}
</span></code></pre></pre>
<p>The reason we need <code>&amp;mut stream</code> here is Windows and <code>IOCP</code>. Our stream holds a
buffer that we'll pass on to Windows. <strong>This buffer must not be touched</strong> while
the OS lends it exclusively. This way we leverage Rusts borrowchecker to help us
make sure of that.</p>
<blockquote>
<p>However, this has a drawback. We don't really need it to be <code>&amp;mut</code> on <code>linux</code> and <code>macos</code>
so on these systems we might get a compiler warning letting us know it doesn't need
to be <code>&amp;mut</code>. There are ways around this though, but in the interest of actually finishing
both books I had to stop somewhere and this is not the worlds end.</p>
</blockquote>
<p>The main point here is that we register an interest to read in a non-blocking manner.</p>
<p>I'll repeat this part of the code so you have it right in front of you while i
explain:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>    let wrapped = move |_n| {
            let mut stream = stream;
            let mut buffer = String::new();
            stream
                .read_to_string(&amp;mut buffer)
                .expect(&quot;Stream read error&quot;);
            cb(Js::String(buffer));
        };
<span class="boring">}
</span></code></pre></pre>
<p>Since our callback expects a <code>Js::String</code> we can't actually pass the buffer. We need
to wrap our callback so we first read out the data to a <code>String</code> which we then
can pass to our code.</p>
<p>Lastly we register the I/O event with our runtime</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>rt.register_event_epoll(token, wrapped);
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#bonus-section" id="bonus-section">Bonus section</a></h2>
<p>Now in a better implementation, we would not issue a blocking call like <code>read_to_string</code>
because it might be that at some point this will block if not all the data we need
is present at once.</p>
<p>The right thing to do then would be to read parts of the
data into a buffer while calling <code>stream.read(..)</code> in a loop, and at some point
this might return an <code>Err::WouldBlock</code> at which we re-wrap our callback and re-register
our read event to read the rest.</p>
<blockquote>
<p>Another reason this might block is that threads might get what is called a &quot;spurious&quot; wakeup.</p>
<p>What is that? Well, the OS might wake up the thread, without there being any data there. The
reason for this is kind of hard to get confirmed, but as far as I understand, this can happen if
the OS is in doubt if an event occurred or not. It does apparently happen that some interrupts
might get issued without the OS getting notified. This can be caused by unfortunate timing since
there are some bits of code that needs to be executed &quot;uninterrupted&quot; and there is a way for the
OS to instruct the CPU to filter some interrupts for short periods. The OS can't be &quot;optimistic&quot; in the
sense that it assumes the event didn't happen since that would cause the process to wait indefinitely
if the event did occur. So instead it might just wake up the thread.</p>
<p>Also, there are performance optimizations
in operating systems that might cause them to choose to wake up waiting threads in a process for unknown
reasons. Therefore, accounting for spurious wakeups is part of the &quot;contract&quot; between the programmer
and the OS.</p>
<p>Either way, the OS assumes we have logic in place to account for this and just re-register our event
if it was a spurious wakeup.</p>
</blockquote>
<p>In our implementation, whether the data is not fully available or if it was a spurious wakeup, we'll end
up blocking. It's fine, we're just trying to understand. We're not re-implementing <code>libuv</code> anyway.</p>
<p>The last part is registering this event with our <code>epoll_thread</code>.</p>
<p>Now, we're practically at the finish line. All the interesting parts are covered, we just
need a few more small things to get it all up and running.</p>
<h1><a class="header" href="#putting-the-pieces-together" id="putting-the-pieces-together">Putting the pieces together</a></h1>
<p>The first step to get this to run is obvious. We need a main function. Fortunately
it's a short one:</p>
<pre><pre class="playpen"><code class="language-rust no_run">fn main() {
    let rt = Runtime::new();
    rt.run(javascript);
}
</code></pre></pre>
<p>We instantiate our runtime, and we pass inn a pointer to our <code>javascript</code> function.</p>
<p>The next parts are just helper methods to let us print out interesting things
about our <code>Runtime</code> while it's running.</p>
<p>For those new to Rust I'll spend the time to explain them anyway:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>fn current() -&gt; String {
    thread::current().name().unwrap().to_string()
}
<span class="boring">}
</span></code></pre></pre>
<p>The <code>current</code> method prints out the name of the current thread it's called from.</p>
<p>Since we have several threads handling tasks for us, this will help us keep track
of what goes on where.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>fn print(t: impl std::fmt::Display) {
    println!(&quot;Thread: {}\t {}&quot;, current(), t);
}
<span class="boring">}
</span></code></pre></pre>
<p>This function takes an argument that implements the <code>Display</code> trait. This trait
defines how we want a type to displayed as text. We call our <code>current</code> function to
get the name of the current thread and outputs it <code>stdout</code>.</p>
<p>The next function us a bit of an introduction to iterators. When we have content
to print we use this one:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>fn print_content(t: impl std::fmt::Display, descr: &amp;str) {
    println!(
        &quot;\n===== THREAD {} START CONTENT - {} =====&quot;,
        current(),
        descr.to_uppercase()
    );

    let content = format!(&quot;{}&quot;, t);
    let lines = content.lines().take(2);
    let main_cont: String = lines.map(|l| format!(&quot;{}\n&quot;, l)).collect();
    let opt_location = content.find(&quot;Location&quot;);

    let opt_location = opt_location.map(|loc| {
        content[loc..]
        .lines()
        .nth(0)
        .map(|l| format!(&quot;{}\n&quot;,l))
        .unwrap_or(String::new())
    });

    println!(
        &quot;{}{}... [Note: Abbreviated for display] ...&quot;,
        main_cont,
        opt_location.unwrap_or(String::new())
    );

    println!(&quot;===== END CONTENT =====\n&quot;);
}

<span class="boring">}
</span></code></pre></pre>
<p>Let's explain the iterators step by step:</p>
<p>First we have:</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let content = format!(&quot;{}&quot;, t);
let lines = content.lines().take(2);
let main_cont: String = lines.map(|l| format!(&quot;{}\n&quot;, l)).collect();
<span class="boring">}
</span></code></pre></pre>
<p>Content is just the content we want to print out. This is tailor made to print
out the interesting parts of our <code>http</code> response, namely the first few lines and
the <code>location</code> header so we know which call it was that got returned.</p>
<p>We only want to print out the two first lines so we get an iterator over those
by calling <code>content.lines().take(2)</code>.</p>
<p>The next step is to create a <code>String</code> out of these two lines.</p>
<p><code>lines.map(|l| format!(&quot;{}\n&quot;, l)).collect();</code> Takes every line and <code>maps</code> them as
a new string which we append <code>\n</code> to to preserve the line breaks (if we don't do
that the lines will come out as a single line). Then we <code>collect</code> that into a <code>String</code>.</p>
<p>We know they collect to a <code>String</code> since we annotated the type of <code>main_cont</code> in <code>let main_cont: String</code>.</p>
<pre><pre class="playpen"><code class="language-rust no_run">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>let opt_location = content.find(&quot;Location&quot;);
let opt_location = opt_location.map(|loc| {
        content[loc..]
        .lines()
        .nth(0)
        .map(|l| format!(&quot;{}\n&quot;,l))
        .unwrap_or(String::new())
    });
<span class="boring">}
</span></code></pre></pre>
<p>The next part finds the location of the &quot;Location&quot; header. <code>find</code> returns an <code>Option</code>,
just keep that in mind.</p>
<p>The next step, we use <code>map</code> again, but this time we use <code>map</code> on an <code>Option</code> type.
In this context, map means that <strong>if</strong> there is <code>Some</code> value, we want to work with that,
which means that <code>map(|loc| {...</code> loc is an index of the where the <code>Location</code> header was found.</p>
<p>Now what we do is that we take a range of our <code>content</code> string starting from the index
of where we found the <code>Location</code> header, and all the way to the end of the string.</p>
<p>From this range we access the iterator over its lines by calling <code>lines()</code>, we take the first
line from this iterator <code>nth(0)</code>. Now again, this returns an <code>Option</code>, so we use <code>map</code>
again to define what we'll do in the case if it's <code>Some</code>.</p>
<p>This means that <strong>if</strong> the first line of <code>content[loc..]</code> is something we pass that
into our closure <code>map(|l| format!(&quot;{}\n&quot;,l))</code>, which results in <code>l</code> as this line.</p>
<p>We simply map this line into a <code>String</code> which we have appended a line break to.</p>
<p>Lastly we <code>unwrap</code> the result of these operations. If you kept your thoungue right
all the way through this should either be Some(the_first_line_starting_with_Location),
or None. If it's None we just pass inn an empty <code>String</code>.</p>
<p>Lastly we output everything.</p>
<h2><a class="header" href="#congratulations" id="congratulations">Congratulations</a></h2>
<p><strong>You made it! Well done my friend! Now, there are a few small chapters left but we're actually done going through all async basics and implementing our runtime. So relax, and give yourself a pat on the back. All the hard work is finished!</strong></p>
<h1><a class="header" href="#final-code" id="final-code">Final code</a></h1>
<p>Remember, for this to run you need to create a cargo binary project, and add the
following dependency to your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
minimio = {git = &quot;https://github.com/cfsamson/examples-io-eventloop&quot;, branch = &quot;master&quot;}
</code></pre>
<p><strong>Important</strong> <br />
In the root of you project, add a file called <code>test.txt</code>, and add the text:</p>
<pre><code>Hello world! This is a text to encrypt!
</code></pre>
<p>What you write in the file is not important. But how many characters is since we
calculate the fibonacchi number based on how many characters there are in the file.</p>
<p>Add to much text and you'll wait for a long time. Of course, you can just change
the logic yourself now to do something else.</p>
<p>In <code>main.rs</code> you can paste in this code:</p>
<pre><pre class="playpen"><code class="language-rust">/// Think of this function as the javascript program you have written
fn javascript() {
    print(&quot;First call to read test.txt&quot;);
    Fs::read(&quot;test.txt&quot;, |result| {
        let text = result.into_string().unwrap();
        let len = text.len();
        print(format!(&quot;First count: {} characters.&quot;, len));

        print(r#&quot;I want to create a &quot;magic&quot; number based on the text.&quot;#);
        Crypto::encrypt(text.len(), |result| {
            let n = result.into_int().unwrap();
            print(format!(r#&quot;&quot;Encrypted&quot; number is: {}&quot;#, n));
        })
    });

    print(&quot;Registering immediate timeout 1&quot;);
    set_timeout(0, |_res| {
        print(&quot;Immediate1 timed out&quot;);
    });
    print(&quot;Registering immediate timeout 2&quot;);
    set_timeout(0, |_res| {
        print(&quot;Immediate2 timed out&quot;);
    });

    // let's read the file again and display the text
    print(&quot;Second call to read test.txt&quot;);
    Fs::read(&quot;test.txt&quot;, |result| {
        let text = result.into_string().unwrap();
        let len = text.len();
        print(format!(&quot;Second count: {} characters.&quot;, len));

        // aaand one more time but not in parallel.
        print(&quot;Third call to read test.txt&quot;);
        Fs::read(&quot;test.txt&quot;, |result| {
            let text = result.into_string().unwrap();
            print_content(&amp;text, &quot;file read&quot;);
        });
    });

    print(&quot;Registering a 3000 and a 500 ms timeout&quot;);
    set_timeout(3000, |_res| {
        print(&quot;3000ms timer timed out&quot;);
        set_timeout(500, |_res| {
            print(&quot;500ms timer(nested) timed out&quot;);
        });
    });

    print(&quot;Registering a 1000 ms timeout&quot;);
    set_timeout(1000, |_res| {
        print(&quot;SETTIMEOUT&quot;);
    });

    // `http_get_slow` let's us define a latency we want to simulate
    print(&quot;Registering http get request to google.com&quot;);
    Http::http_get_slow(&quot;http//www.google.com&quot;, 2000, |result| {
        let result = result.into_string().unwrap();
        print_content(result.trim(), &quot;web call&quot;);
    });
}

fn main() {
    let rt = Runtime::new();
    rt.run(javascript);
}

// ===== THIS IS OUR &quot;NODE LIBRARY&quot; =====
use std::collections::{BTreeMap, HashMap};
use std::fmt;
use std::fs;
use std::io::{self, Read, Write};
use std::sync::mpsc::{channel, Receiver, Sender};
use std::sync::{Arc, Mutex};
use std::thread::{self, JoinHandle};
use std::time::{Duration, Instant};

use minimio;

static mut RUNTIME: *mut Runtime = std::ptr::null_mut();

struct Task {
    task: Box&lt;dyn Fn() -&gt; Js + Send + 'static&gt;,
    callback_id: usize,
    kind: ThreadPoolTaskKind,
}

impl Task {
    fn close() -&gt; Self {
        Task {
            task: Box::new(|| Js::Undefined),
            callback_id: 0,
            kind: ThreadPoolTaskKind::Close,
        }
    }
}

pub enum ThreadPoolTaskKind {
    FileRead,
    Encrypt,
    Close,
}

impl fmt::Display for ThreadPoolTaskKind {
    fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {
        use ThreadPoolTaskKind::*;
        match self {
            FileRead =&gt; write!(f, &quot;File read&quot;),
            Encrypt =&gt; write!(f, &quot;Encrypt&quot;),
            Close =&gt; write!(f, &quot;Close&quot;),
        }
    }
}

#[derive(Debug)]
pub enum Js {
    Undefined,
    String(String),
    Int(usize),
}

impl Js {
    /// Convenience method since we know the types
    fn into_string(self) -&gt; Option&lt;String&gt; {
        match self {
            Js::String(s) =&gt; Some(s),
            _ =&gt; None,
        }
    }

    /// Convenience method since we know the types
    fn into_int(self) -&gt; Option&lt;usize&gt; {
        match self {
            Js::Int(n) =&gt; Some(n),
            _ =&gt; None,
        }
    }
}

/// NodeTheread represents a thread in our threadpool. Each event has a Joinhandle
/// and a transmitter part of a channel which is used to inform our main loop
/// about what events has occurred.
#[derive(Debug)]
struct NodeThread {
    pub(crate) handle: JoinHandle&lt;()&gt;,
    sender: Sender&lt;Task&gt;,
}

pub struct Runtime {
    /// Available threads for the threadpool
    available_threads: Vec&lt;usize&gt;,
    /// Callbacks scheduled to run
    callbacks_to_run: Vec&lt;(usize, Js)&gt;,
    /// All registered callbacks
    callback_queue: HashMap&lt;usize, Box&lt;dyn FnOnce(Js)&gt;&gt;,
    /// Number of pending epoll events, only used by us to print for this example
    epoll_pending_events: usize,
    /// Our event registrator which registers interest in events with the OS
    epoll_registrator: minimio::Registrator,
    // The handle to our epoll thread
    epoll_thread: thread::JoinHandle&lt;()&gt;,
    /// None = infinite, Some(n) = timeout in n ms, Some(0) = immediate
    epoll_timeout: Arc&lt;Mutex&lt;Option&lt;i32&gt;&gt;&gt;,
    /// Channel used by both our threadpool and our epoll thread to send events
    /// to the main loop
    event_reciever: Receiver&lt;PollEvent&gt;,
    /// Creates an unique identity for our callbacks
    identity_token: usize,
    /// The number of events pending. When this is zero, we're done
    pending_events: usize,
    /// Handles to our threads in the threadpool
    thread_pool: Vec&lt;NodeThread&gt;,
    /// Holds all our timers, and an Id for the callback to run once they expire
    timers: BTreeMap&lt;Instant, usize&gt;,
    /// A struct to temporarely hold timers to remove. We let Runtinme have
    /// ownership so we can reuse the same memory
    timers_to_remove: Vec&lt;Instant&gt;,
}

/// Describes the three main events our epoll-eventloop handles
enum PollEvent {
    /// An event from the `threadpool` with a tuple containing the `thread id`,
    /// the `callback_id` and the data which the we expect to process in our
    /// callback
    Threadpool((usize, usize, Js)),
    /// An event from the epoll-based eventloop holding the `event_id` for the
    /// event
    Epoll(usize),
    Timeout,
}

impl Runtime {
    pub fn new() -&gt; Self {
        // ===== THE REGULAR THREADPOOL =====
        let (event_sender, event_reciever) = channel::&lt;PollEvent&gt;();
        let mut threads = Vec::with_capacity(4);

        for i in 0..4 {
            let (evt_sender, evt_reciever) = channel::&lt;Task&gt;();
            let event_sender = event_sender.clone();

            let handle = thread::Builder::new()
                .name(format!(&quot;pool{}&quot;, i))
                .spawn(move || {

                    while let Ok(task) = evt_reciever.recv() {
                        print(format!(&quot;received a task of type: {}&quot;, task.kind));

                        if let ThreadPoolTaskKind::Close = task.kind {
                            break;
                        };

                        let res = (task.task)();
                        print(format!(&quot;finished running a task of type: {}.&quot;, task.kind));

                        let event = PollEvent::Threadpool((i, task.callback_id, res));
                        event_sender.send(event).expect(&quot;threadpool&quot;);
                    }
                })
                .expect(&quot;Couldn't initialize thread pool.&quot;);

            let node_thread = NodeThread {
                handle,
                sender: evt_sender,
            };

            threads.push(node_thread);
        }

        // ===== EPOLL THREAD =====
        let mut poll = minimio::Poll::new().expect(&quot;Error creating epoll queue&quot;);
        let registrator = poll.registrator();
        let epoll_timeout = Arc::new(Mutex::new(None));
        let epoll_timeout_clone = epoll_timeout.clone();

        let epoll_thread = thread::Builder::new()
            .name(&quot;epoll&quot;.to_string())
            .spawn(move || {
                let mut events = minimio::Events::with_capacity(1024);

                loop {
                    let epoll_timeout_handle = epoll_timeout_clone.lock().unwrap();
                    let timeout = *epoll_timeout_handle;
                    drop(epoll_timeout_handle);

                    match poll.poll(&amp;mut events, timeout) {
                        Ok(v) if v &gt; 0 =&gt; {
                            for i in 0..v {
                                let event = events.get_mut(i).expect(&quot;No events in event list.&quot;);
                                print(format!(&quot;epoll event {} is ready&quot;, event.id()));

                                let event = PollEvent::Epoll(event.id());
                                event_sender.send(event).expect(&quot;epoll event&quot;);
                            }
                        }
                        Ok(v) if v == 0 =&gt; {
                            print(&quot;epoll event timeout is ready&quot;);
                            event_sender.send(PollEvent::Timeout).expect(&quot;epoll timeout&quot;);
                        }
                        Err(ref e) if e.kind() == io::ErrorKind::Interrupted =&gt; {
                            print(&quot;received event of type: Close&quot;);
                            break;
                        }
                        Err(e) =&gt; panic!(&quot;{:?}&quot;, e),
                        _ =&gt; unreachable!(),
                    }
                }
            })
            .expect(&quot;Error creating epoll thread&quot;);

        Runtime {
            available_threads: (0..4).collect(),
            callbacks_to_run: vec![],
            callback_queue: HashMap::new(),
            epoll_pending_events: 0,
            epoll_registrator: registrator,
            epoll_thread,
            epoll_timeout,
            event_reciever,
            identity_token: 0,
            pending_events: 0,
            thread_pool: threads,
            timers: BTreeMap::new(),
            timers_to_remove: vec![],
        }
    }

    /// This is the event loop. There are several things we could do here to
    /// make it a better implementation. One is to set a max backlog of callbacks
    /// to execute in a single tick, so we don't starve the threadpool or file
    /// handlers. Another is to dynamically decide if/and how long the thread
    /// could be allowed to be parked for example by looking at the backlog of
    /// events, and if there is any backlog disable it. Some of our Vec's will
    /// only grow, and not resize, so if we have a period of very high load, the
    /// memory will stay higher than we need until a restart. This could be
    /// dealt by using a different kind of data structure like a `LinkedList`.
    pub fn run(mut self, f: impl Fn()) {
        let rt_ptr: *mut Runtime = &amp;mut self;
        unsafe { RUNTIME = rt_ptr };

        // just for us priting out during execution
        let mut ticks = 0;

        // First we run our &quot;main&quot; function
        f();

        // ===== EVENT LOOP =====
        while self.pending_events &gt; 0 {
            ticks += 1;
            // NOT PART OF LOOP, JUST FOR US TO SEE WHAT TICK IS EXCECUTING
            print(format!(&quot;===== TICK {} =====&quot;, ticks));

            // ===== 1. TIMERS =====
            self.process_expired_timers();

            // ===== 2. CALLBACKS =====
            // Timer callbacks and if for some reason we have postponed callbacks
            // to run on the next tick. Not possible in our implementation though.
            self.run_callbacks();

            // ===== 3. IDLE/PREPARE =====
            // we won't use this

            // ===== 4. POLL =====
            // First we need to check if we have any outstanding events at all
            // and if not we're finished. If not we will wait forever.
            if self.pending_events == 0 {
                break;
            }

            // We want to get the time to the next timeout (if any) and we
            // set the timeout of our epoll wait to the same as the timeout
            // for the next timer. If there is none, we set it to infinite (None)
            let next_timeout = self.get_next_timer();

            let mut epoll_timeout_lock = self.epoll_timeout.lock().unwrap();
            *epoll_timeout_lock = next_timeout;
            // We release the lock before we wait in `recv`
            drop(epoll_timeout_lock);

            // We handle one and one event but multiple events could be returned
            // on the same poll. We won't cover that here though but there are
            // several ways of handling this.
            if let Ok(event) = self.event_reciever.recv() {
                match event {
                    PollEvent::Timeout =&gt; (),
                    PollEvent::Threadpool((thread_id, callback_id, data)) =&gt; {
                        self.process_threadpool_events(thread_id, callback_id, data);
                    }
                    PollEvent::Epoll(event_id) =&gt; {
                        self.process_epoll_events(event_id);
                    }
                }
            }
            self.run_callbacks();

            // ===== 5. CHECK =====
            // an set immediate function could be added pretty easily but we
            // won't do that here

            // ===== 6. CLOSE CALLBACKS ======
            // Release resources, we won't do that here, but this is typically
            // where sockets etc are closed.
        }

        // We clean up our resources, makes sure all destructors run.
        for thread in self.thread_pool.into_iter() {
            thread.sender.send(Task::close()).expect(&quot;threadpool cleanup&quot;);
            thread.handle.join().unwrap();
        }

        self.epoll_registrator.close_loop().unwrap();
        self.epoll_thread.join().unwrap();

        print(&quot;FINISHED&quot;);
    }

    fn process_expired_timers(&amp;mut self) {
        // Need an intermediate variable to please the borrowchecker
        let timers_to_remove = &amp;mut self.timers_to_remove;

        self.timers
            .range(..=Instant::now())
            .for_each(|(k, _)| timers_to_remove.push(*k));

        while let Some(key) = self.timers_to_remove.pop() {
            let callback_id = self.timers.remove(&amp;key).unwrap();
            self.callbacks_to_run.push((callback_id, Js::Undefined));
        }
    }

    fn get_next_timer(&amp;self) -&gt; Option&lt;i32&gt; {
        self.timers.iter().nth(0).map(|(&amp;instant, _)| {
            let mut time_to_next_timeout = instant - Instant::now();

            if time_to_next_timeout &lt; Duration::new(0, 0) {
                time_to_next_timeout = Duration::new(0, 0);
            }

            time_to_next_timeout.as_millis() as i32
        })
    }

    fn run_callbacks(&amp;mut self) {
        while let Some((callback_id, data)) = self.callbacks_to_run.pop() {
            let cb = self.callback_queue.remove(&amp;callback_id).unwrap();
            cb(data);
            self.pending_events -= 1;
        }
    }

    fn process_epoll_events(&amp;mut self, event_id: usize) {
        self.callbacks_to_run.push((event_id, Js::Undefined));
        self.epoll_pending_events -= 1;
    }

    fn process_threadpool_events(&amp;mut self, thread_id: usize, callback_id: usize, data: Js) {
        self.callbacks_to_run.push((callback_id, data));
        self.available_threads.push(thread_id);
    }

    fn get_available_thread(&amp;mut self) -&gt; usize {
        match self.available_threads.pop() {
            Some(thread_id) =&gt; thread_id,
            // We would normally return None and the request and not panic!
            None =&gt; panic!(&quot;Out of threads.&quot;),
        }
    }

    /// If we hit max we just wrap around
    fn generate_identity(&amp;mut self) -&gt; usize {
        self.identity_token = self.identity_token.wrapping_add(1);
        self.identity_token
    }

    fn generate_cb_identity(&amp;mut self) -&gt; usize {
        let ident = self.generate_identity();
        let taken = self.callback_queue.contains_key(&amp;ident);

        // if there is a collision or the identity is already there we loop until we find a new one
        // we don't cover the case where there are `usize::MAX` number of callbacks waiting since
        // that if we're fast and queue a new event every nanosecond that will still take 585.5 years
        // to do on a 64 bit system.
        if !taken {
            ident
        } else {
            loop {
                let possible_ident = self.generate_identity();
                if self.callback_queue.contains_key(&amp;possible_ident) {
                    break possible_ident;
                }
            }
        }
    }

    /// Adds a callback to the queue and returns the key
    fn add_callback(&amp;mut self, ident: usize, cb: impl FnOnce(Js) + 'static) {
        let boxed_cb = Box::new(cb);
        self.callback_queue.insert(ident, boxed_cb);
    }

    pub fn register_event_epoll(&amp;mut self, token: usize, cb: impl FnOnce(Js) + 'static) {
        self.add_callback(token, cb);

        print(format!(&quot;Event with id: {} registered.&quot;, token));
        self.pending_events += 1;
        self.epoll_pending_events += 1;
    }

    pub fn register_event_threadpool(
        &amp;mut self,
        task: impl Fn() -&gt; Js + Send + 'static,
        kind: ThreadPoolTaskKind,
        cb: impl FnOnce(Js) + 'static,
    ) {
        let callback_id = self.generate_cb_identity();
        self.add_callback(callback_id, cb);

        let event = Task {
            task: Box::new(task),
            callback_id,
            kind,
        };

        // we are not going to implement a real scheduler here, just a LIFO queue
        let available = self.get_available_thread();
        self.thread_pool[available].sender.send(event).expect(&quot;register work&quot;);
        self.pending_events += 1;
    }

    fn set_timeout(&amp;mut self, ms: u64, cb: impl Fn(Js) + 'static) {
        // Is it theoretically possible to get two equal instants? If so we'll have a bug...
        let now = Instant::now();

        let cb_id = self.generate_cb_identity();
        self.add_callback(cb_id, cb);

        let timeout = now + Duration::from_millis(ms);
        self.timers.insert(timeout, cb_id);

        self.pending_events += 1;
        print(format!(&quot;Registered timer event id: {}&quot;, cb_id));
    }
}

pub fn set_timeout(ms: u64, cb: impl Fn(Js) + 'static) {
    let rt = unsafe { &amp;mut *(RUNTIME as *mut Runtime) };
    rt.set_timeout(ms, cb);
}

// ===== THIS IS PLUGINS CREATED IN C++ FOR THE NODE RUNTIME OR PART OF THE RUNTIME ITSELF =====
// The pointer dereferencing of our runtime is not striclty needed but is mostly for trying to
// emulate a bit of the same feeling as when you use modules in javascript. We could pass the runtime in
// as a reference to our startup function.

struct Crypto;
impl Crypto {
    fn encrypt(n: usize, cb: impl Fn(Js) + 'static + Clone) {
        let work = move || {
            fn fibonacchi(n: usize) -&gt; usize {
                match n {
                    0 =&gt; 0,
                    1 =&gt; 1,
                    _ =&gt; fibonacchi(n - 1) + fibonacchi(n - 2),
                }
            }

            let fib = fibonacchi(n);
            Js::Int(fib)
        };

        let rt = unsafe { &amp;mut *RUNTIME };
        rt.register_event_threadpool(work, ThreadPoolTaskKind::Encrypt, cb);
    }
}

struct Fs;
impl Fs {
    fn read(path: &amp;'static str, cb: impl Fn(Js) + 'static) {
        let work = move || {
            // Let's simulate that there is a very large file we're reading allowing us to actually
            // observe how the code is executed
            thread::sleep(std::time::Duration::from_secs(1));
            let mut buffer = String::new();
            fs::File::open(&amp;path)
                .unwrap()
                .read_to_string(&amp;mut buffer)
                .unwrap();
            Js::String(buffer)
        };
        let rt = unsafe { &amp;mut *RUNTIME };
        rt.register_event_threadpool(work, ThreadPoolTaskKind::FileRead, cb);
    }
}

struct Http;
impl Http {
    pub fn http_get_slow(url: &amp;str, delay_ms: u32, cb: impl Fn(Js) + 'static + Clone) {
        let rt: &amp;mut Runtime = unsafe { &amp;mut *RUNTIME };

        // Don't worry, http://slowwly.robertomurray.co.uk is a site for simulating a delayed
        // response from a server. Perfect for our use case.
        let adr = &quot;slowwly.robertomurray.co.uk:80&quot;;
        let mut stream = minimio::TcpStream::connect(adr).unwrap();
        let request = format!(
            &quot;GET /delay/{}/url/http://{} HTTP/1.1\r\n\
             Host: slowwly.robertomurray.co.uk\r\n\
             Connection: close\r\n\
             \r\n&quot;,
            delay_ms, url
        );

        stream
            .write_all(request.as_bytes())
            .expect(&quot;Error writing to stream&quot;);

        let token = rt.generate_cb_identity();

        rt.epoll_registrator
            .register(&amp;mut stream, token, minimio::Interests::READABLE)
            .unwrap();

        let wrapped = move |_n| {
            let mut stream = stream;
            let mut buffer = String::new();

            stream
                .read_to_string(&amp;mut buffer)
                .expect(&quot;Stream read error&quot;);

            cb(Js::String(buffer));
        };

        rt.register_event_epoll(token, wrapped);
    }
}

fn print(t: impl std::fmt::Display) {
    println!(&quot;Thread: {}\t {}&quot;, current(), t);
}

fn print_content(t: impl std::fmt::Display, descr: &amp;str) {
    println!(
        &quot;\n===== THREAD {} START CONTENT - {} =====&quot;,
        current(),
        descr.to_uppercase()
    );

    let content = format!(&quot;{}&quot;, t);
    let lines = content.lines().take(2);
    let main_cont: String = lines.map(|l| format!(&quot;{}\n&quot;, l)).collect();
    let opt_location = content.find(&quot;Location&quot;);

    let opt_location = opt_location.map(|loc| {
        content[loc..]
        .lines()
        .nth(0)
        .map(|l| format!(&quot;{}\n&quot;,l))
        .unwrap_or(String::new())
    });

    println!(
        &quot;{}{}... [Note: Abbreviated for display] ...&quot;,
        main_cont,
        opt_location.unwrap_or(String::new())
    );

    println!(&quot;===== END CONTENT =====\n&quot;);
}

fn current() -&gt; String {
    thread::current().name().unwrap().to_string()
}

</code></pre></pre>
<h1><a class="header" href="#shortcuts-and-improvements" id="shortcuts-and-improvements">Shortcuts and improvements</a></h1>
<p>There are several things we could do here to make this a better implementation which I want to point out here.</p>
<p>One is to set a max backlog of callbacks to execute in a single tick, so we don't starve the threadpool or file handlers. In other words, limit the amount of callbacks we execute in a single iteration.</p>
<p>Another is to dynamically decide if/and how long the thread could be allowed to be parked for example by looking at the backlog of events, and if there is any backlog disable any waiting at all.</p>
<p>When it comes to data structures, there is lots we could have done.
If you look closely, you'll see that some of our Vec's will only grow, and not resize ever, so if we have a period of very high load,the memory will stay higher than it needs to be until a restart. This could be dealt with or a different data structure without this property could have been chosen (like a linked list).</p>
<p>Some of the shortcuts (like panicing when we have no available threads in the threadpool) is especially limiting. If you were to improve this example, that would be one of the first things to fix.</p>
<p>There is probably several other aspects that deserve to be mentioned here, but I'll leave it with this for now.</p>
<h1><a class="header" href="#conclusion" id="conclusion">Conclusion</a></h1>
<p>Congratulations!!</p>
<p>What a ride, huh? We're finished, take a break and get some fresh air. You've deserved
it.</p>
<p>We did cover a lot in this book in a fairly condensed manner. I hope I kept my
initial promise of what we were going to learn and that you also got something
useful out of this.</p>
<p>Anyway, I want to thank you for reading all the way to the end, and I really do hope you enjoyed it.</p>
<p>Feel free to visit the <a href="https://github.com/cfsamson/book-exploring-async-basics">repository of this book</a> or the <a href="https://github.com/cfsamson/examples-node-eventloop">repository for our example code</a>. I would love to hear from you
if you think there is improvements I could make or if you just found it a good read.</p>
<p>Until next time!</p>
<p>Carl Fredrik Samson</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        
        <!-- Google Analytics Tag -->
        <script type="text/javascript">
            var localAddrs = ["localhost", "127.0.0.1", ""];

            // make sure we don't activate google analytics if the developer is
            // inspecting the book locally...
            if (localAddrs.indexOf(document.location.hostname) === -1) {
                (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

                ga('create', 'UA-149686420-1', 'auto');
                ga('send', 'pageview');
            }
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
